{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Q43JtzGbvfc2",
      "metadata": {
        "id": "Q43JtzGbvfc2"
      },
      "source": [
        "### <font color='#4285f4'>Overview</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wP2PecnQvnVg",
      "metadata": {
        "id": "wP2PecnQvnVg"
      },
      "source": [
        "Overview: This notebook will call Gemini to describe the code, and then call Data Engineering agent to create BigQuery pipeline or dataform pipeline\n",
        "\n",
        "Cost:\n",
        "\n",
        "Approximate cost: $1\n",
        "\n",
        "Author:\n",
        "\n",
        "Navjot Singh\n",
        "\n",
        "Adam Paternostro"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zn-HFfWFxnAB",
      "metadata": {
        "id": "Zn-HFfWFxnAB"
      },
      "source": [
        "### <font color='#4285f4'>License</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "I_tP0bP5xpjd",
      "metadata": {
        "executionInfo": {
          "elapsed": 5,
          "status": "ok",
          "timestamp": 1758147837822,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "I_tP0bP5xpjd"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4cE25OPw1mt-",
      "metadata": {
        "executionInfo": {
          "elapsed": 198,
          "status": "ok",
          "timestamp": 1758148853860,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "4cE25OPw1mt-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.auth\n",
        "from PIL import Image\n",
        "from IPython.display import HTML\n",
        "import IPython.display\n",
        "import google.auth\n",
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "import base64\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import base64\n",
        "import random\n",
        "from google.auth.transport import requests\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "\n",
        "\n",
        "import logging\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, before_sleep_log, retry_if_exception\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "location = \"us-central1\"\n",
        "d_location=\"us-central1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FIMkWrSD8Ymr",
      "metadata": {
        "id": "FIMkWrSD8Ymr"
      },
      "source": [
        "# PySpark Code which reads data from multiple BigQuery Tables and writes into a denormalized table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "KggsSQjM8SEc",
      "metadata": {
        "executionInfo": {
          "elapsed": 179,
          "status": "ok",
          "timestamp": 1758148209304,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "KggsSQjM8SEc"
      },
      "outputs": [],
      "source": [
        "sample_pyspark_code=f\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"DenormalizedOrderItemDetails\").getOrCreate()\n",
        "\n",
        "# Define the BigQuery project ID (replace with your actual project ID)\n",
        "# Assuming 'ecommerce' is your project containing the datasets.\n",
        "# You might need to adjust this if your tables are in a different project.\n",
        "bigquery_project_id = \"{project_id}\"\n",
        "temporary_gcs_bucket = \"test\" # Replace with your GCS temporary bucket\n",
        "\n",
        "# Read the tables from BigQuery\n",
        "order_items_df = spark.read.format(\"bigquery\") \\\n",
        "  .option(\"table\", f\"{project_id}.ecommerce.order_items\") \\\n",
        "  .load()\n",
        "\n",
        "orders_df = spark.read.format(\"bigquery\") \\\n",
        "  .option(\"table\", f\"{project_id}.ecommerce.orders\") \\\n",
        "  .load()\n",
        "\n",
        "users_df = spark.read.format(\"bigquery\") \\\n",
        "  .option(\"table\", f\"{project_id}.ecommerce.users\") \\\n",
        "  .load()\n",
        "\n",
        "products_df = spark.read.format(\"bigquery\") \\\n",
        "  .option(\"table\", f\"{project_id}.ecommerce.products\") \\\n",
        "  .load()\n",
        "\n",
        "# Perform the joins\n",
        "# Join order_items with orders\n",
        "joined_df = order_items_df.alias(\"oi\").join(\n",
        "    orders_df.alias(\"o\"),\n",
        "    col(\"oi.order_id\") == col(\"o.order_id\"),\n",
        "    \"inner\"\n",
        ")\n",
        "\n",
        "# Join the result with users\n",
        "joined_df = joined_df.join(\n",
        "    users_df.alias(\"u\"),\n",
        "    col(\"oi.user_id\") == col(\"u.id\"),\n",
        "    \"inner\"\n",
        ")\n",
        "\n",
        "# Join the result with products\n",
        "denormalized_order_item_details_df = joined_df.join(\n",
        "    products_df.alias(\"p\"),\n",
        "    col(\"oi.product_id\") == col(\"p.id\"),\n",
        "    \"inner\"\n",
        ").select(\n",
        "    col(\"oi.id\").alias(\"order_item_id\"),\n",
        "    col(\"o.order_id\"),\n",
        "    col(\"o.user_id\"),\n",
        "    col(\"p.id\").alias(\"product_id\"),\n",
        "    col(\"u.first_name\"),\n",
        "    col(\"u.last_name\"),\n",
        "    col(\"u.email\"),\n",
        "    col(\"u.age\"),\n",
        "    col(\"u.gender\"),\n",
        "    col(\"u.state\"),\n",
        "    col(\"u.street_address\"),\n",
        "    col(\"u.postal_code\"),\n",
        "    col(\"u.city\"),\n",
        "    col(\"u.country\"),\n",
        "    col(\"u.traffic_source\"),\n",
        "    col(\"p.name\").alias(\"product_name\"),\n",
        "    col(\"p.brand\").alias(\"product_brand\"),\n",
        "    col(\"p.category\").alias(\"product_category\"),\n",
        "    col(\"p.department\").alias(\"product_department\"),\n",
        "    col(\"p.retail_price\").alias(\"product_retail_price\"),\n",
        "    col(\"oi.status\"),\n",
        "    col(\"oi.created_at\"),\n",
        "    col(\"oi.shipped_at\"),\n",
        "    col(\"oi.delivered_at\"),\n",
        "    col(\"oi.returned_at\"),\n",
        "    col(\"oi.sale_price\")\n",
        ")\n",
        "\n",
        "# Write the result to a new BigQuery table\n",
        "# The target table name is 'ecommerce.denormalized_order_item_details'\n",
        "output_table = f\"{project_id}.ecommerce.denormalized_order_item_details\"\n",
        "\n",
        "denormalized_order_item_details_df.write.format(\"bigquery\") \\\n",
        "  .option(\"temporaryGcsBucket\", temporary_gcs_bucket) \\\n",
        "  .option(\"table\", output_table) \\\n",
        "  .mode(\"overwrite\").save()\n",
        "\n",
        "spark.stop()\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WSYNfLKEGpmP",
      "metadata": {
        "id": "WSYNfLKEGpmP"
      },
      "source": [
        "# Rest API Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8Vfglc_WE2tX",
      "metadata": {
        "executionInfo": {
          "elapsed": 178,
          "status": "ok",
          "timestamp": 1758148212918,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "8Vfglc_WE2tX"
      },
      "outputs": [],
      "source": [
        "def rest_api_helper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import google.auth.transport.requests\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "      return json.loads(response.content)\n",
        "  else:\n",
        "    error = f\"Error rest_api_helper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T-918zkcWVD2",
      "metadata": {
        "id": "T-918zkcWVD2"
      },
      "source": [
        "# Account impersonation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f9i4ShGQWQCI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2723,
          "status": "ok",
          "timestamp": 1758148218222,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "f9i4ShGQWQCI",
        "outputId": "bcb0dcc3-192d-4f6b-ce7f-4a213f35636b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated IAM policy for serviceAccount [879468126116-compute@developer.gserviceaccount.com].\n",
            "bindings:\n",
            "- members:\n",
            "  - serviceAccount:service-879468126116@gcp-sa-dataform.iam.gserviceaccount.com\n",
            "  role: roles/iam.serviceAccountTokenCreator\n",
            "etag: BwY_Bsf6NgE=\n",
            "version: 1\n"
          ]
        }
      ],
      "source": [
        "project_number_list = !gcloud projects describe {project_id} --format=\"value(projectNumber)\"\n",
        "project_number = project_number_list[0]\n",
        "command=f\"\"\"gcloud iam service-accounts add-iam-policy-binding {project_number}-compute@developer.gserviceaccount.com --member=\"serviceAccount:service-{project_number}@gcp-sa-dataform.iam.gserviceaccount.com\" --role='roles/iam.serviceAccountTokenCreator'\"\"\"\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3_JkbOZDZLh-",
      "metadata": {
        "id": "3_JkbOZDZLh-"
      },
      "source": [
        "# Define Variables and get project number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ExTDzCdE_q-g",
      "metadata": {
        "id": "ExTDzCdE_q-g"
      },
      "outputs": [],
      "source": [
        "repo_name=\"spark-data-eng25\"\n",
        "pipeline_name=\"spark-pipeline25\"\n",
        "display_name=\"spark-pipeline25\"\n",
        "workspace_id=\"spark_pipeline3\"\n",
        "dataform_region = location\n",
        "service_account = f\"\"\"service-{project_number}@gcp-sa-dataform.iam.gserviceaccount.com\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ERRezZd6O7e",
      "metadata": {
        "id": "0ERRezZd6O7e"
      },
      "source": [
        "# Function to Create BigQuery Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UCeylGFCVPvC",
      "metadata": {
        "id": "UCeylGFCVPvC"
      },
      "outputs": [],
      "source": [
        "def create_bigquery_pipeline(pipeline_name: str, display_name:str,dataform_region:str, service_account:str, project_id) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Creates a new Dataform repository, referred to as a BigQuery Pipeline, if it does not already exist.\n",
        "\n",
        "    This function uses a specific label '{\"bigquery-workflow\":\"preview\"}' which\n",
        "    is a temporary method until an official API is released for this functionality.\n",
        "\n",
        "    Args:\n",
        "        pipeline_name (str): The name/ID for the new repository. This will be used as the\n",
        "                    repositoryId,  and name.\n",
        "        display_name (str): The display name seen in the user interface.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and results of the operation.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"create_bigquery_pipeline\",\n",
        "            \"query\": None,\n",
        "            \"created\": True / False if the workspace was created,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from Dataform ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\")\n",
        "    #service_account = os.getenv(\"AGENT_ENV_DATAFORM_SERVICE_ACCOUNT\")\n",
        "\n",
        "    # Check if the repository already exists before attempting to create it.\n",
        "    #existence_check = exists_dataform_repository(pipeline_name) # Added await\n",
        "    #messages = existence_check.get(\"messages\", [])\n",
        "\n",
        "    #if existence_check[\"status\"] == \"failed\":\n",
        "    #    return existence_check\n",
        "\n",
        "    #if existence_check[\"results\"][\"exists\"]:\n",
        "        # If the repository exists, return a success message indicating it wasn't re-created.\n",
        "    #   return {\n",
        "    #        \"status\": \"success\",\n",
        "    #        \"tool_name\": \"create_bigquery_pipeline\",\n",
        "    #        \"query\": None,\n",
        "    #        \"created\": False,\n",
        "    #        \"messages\": messages,\n",
        "    #        \"results\": {\"name\": existence_check[\"results\"][\"name\"]}\n",
        "    #    }\n",
        "    messages=[]\n",
        "    messages.append(f\"Creating BigQuery Pipeline (Dataform Repository) '{pipeline_name}' in region '{dataform_region}'.\")\n",
        "    # The repositoryId is passed as a query parameter. [2]\n",
        "    url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories?repositoryId={pipeline_name}\"\n",
        "\n",
        "    # The request body contains the configuration for the new repository.\n",
        "    request_body = {\n",
        "        \"serviceAccount\": service_account,\n",
        "        \"displayName\": display_name,\n",
        "        \"name\": pipeline_name,\n",
        "        # This label is a temporary \"hack\" until a formal API is available.\n",
        "        \"labels\": {\"bigquery-workflow\": \"preview\"}\n",
        "    }\n",
        "\n",
        "    #logger.debug(f\"request_body: {request_body}\")\n",
        "\n",
        "    try:\n",
        "        # Call the REST API helper to execute the POST request. [2]\n",
        "        json_result = rest_api_helper(url, \"POST\", request_body) # Added await\n",
        "\n",
        "        messages.append(f\"Successfully initiated the creation of repository '{pipeline_name}'.\")\n",
        "    #    logger.debug(f\"create_bigquery_pipeline json_result: {json_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"create_bigquery_pipeline\",\n",
        "            \"query\": None,\n",
        "            \"created\": True,\n",
        "            \"messages\": messages,\n",
        "            \"results\": json_result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while creating the BigQuery Pipeline '{pipeline_name}': {e}\"\n",
        "        messages.append(error_message)\n",
        "    #    logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"create_bigquery_pipeline\",\n",
        "            \"query\": None,\n",
        "            \"created\": False,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1sjXnVt3Rm18",
      "metadata": {
        "id": "1sjXnVt3Rm18"
      },
      "source": [
        "# Function to create Dataform Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t9mvibh4RlPU",
      "metadata": {
        "id": "t9mvibh4RlPU"
      },
      "outputs": [],
      "source": [
        "def create_dataform_pipeline(repository_id: str, display_name: str, dataform_region, service_account,project_id) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Creates a new, standard Dataform repository if it does not already exist.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name/ID for the new repository. This will be used as the\n",
        "                    repositoryId and name.\n",
        "        display_name (str): The display name for the repository.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and results of the operation.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"create_dataform_pipeline\",\n",
        "            \"query\": None,\n",
        "            \"created\": True / False if the workspace was created,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from Dataform ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\")\n",
        "    #service_account = os.getenv(\"AGENT_ENV_DATAFORM_SERVICE_ACCOUNT\")\n",
        "\n",
        "    # Check if the repository already exists before attempting to create it.\n",
        "    #existence_check = exists_dataform_repository(repository_id) # Added await\n",
        "    messages = []\n",
        "\n",
        "\n",
        "    messages.append(f\"Creating standard Dataform Repository '{repository_id}' in region '{dataform_region}'.\")\n",
        "    # The repositoryId is passed as a query parameter. [2]\n",
        "    url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories?repositoryId={repository_id}\"\n",
        "\n",
        "    # The request body for a standard Dataform repository.\n",
        "    request_body = {\n",
        "        \"serviceAccount\": service_account,\n",
        "        \"displayName\": display_name,\n",
        "        \"name\": repository_id,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Call the REST API helper to execute the POST request. [2]\n",
        "        json_result = rest_api_helper(url, \"POST\", request_body) # Added await\n",
        "\n",
        "        messages.append(f\"Successfully initiated the creation of repository '{repository_id}'.\")\n",
        "        #logger.debug(f\"create_dataform_pipeline json_result: {json_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"create_dataform_pipeline\",\n",
        "            \"query\": None,\n",
        "            \"created\": True,\n",
        "            \"messages\": messages,\n",
        "            \"results\": json_result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while creating the Dataform repository '{repository_id}': {e}\"\n",
        "        messages.append(error_message)\n",
        "        #logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"create_dataform_pipeline\",\n",
        "            \"query\": None,\n",
        "            \"created\": False,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T3UHGV3B6aRM",
      "metadata": {
        "id": "T3UHGV3B6aRM"
      },
      "source": [
        "# Function to Create Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hk-AMVOve8cu",
      "metadata": {
        "id": "Hk-AMVOve8cu"
      },
      "outputs": [],
      "source": [
        "def create_workspace(repository_id: str, workspace_id: str, dataform_region:str, project_id) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Creates a new Dataform workspace in a repository if it does not already exist.\n",
        "\n",
        "    Args:\n",
        "        repository_id (str): The ID of the repository where the workspace will be created.\n",
        "        workspace_id (str): The ID for the new workspace.  The workspace display name will also be the workspace id.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and results of the operation.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"create_workspace\",\n",
        "            \"query\": None,\n",
        "            \"created\": True / False if the workspace was created,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from Dataform ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "\n",
        "    # Check if the workspace already exists before attempting to create it.\n",
        "    #existence_check = exists_dataform_workspace(repository_id, workspace_id) # Added await\n",
        "    #messages = existence_check.get(\"messages\", [])\n",
        "\n",
        "    #if existence_check[\"status\"] == \"failed\":\n",
        "    #    return existence_check\n",
        "\n",
        "    #if existence_check[\"results\"][\"exists\"]:\n",
        "    #    # If the workspace exists, return a success message indicating it wasn't re-created.\n",
        "    #    return {\n",
        "    #        \"status\": \"success\",\n",
        "    #        \"tool_name\": \"create_workspace\",\n",
        "    #        \"query\": None,\n",
        "    #        \"created\": False,\n",
        "    #        \"messages\": messages,\n",
        "    #        \"results\": {\"name\": existence_check[\"results\"][\"name\"]}\n",
        "    #    }\n",
        "\n",
        "    #messages.append(f\"Creating workspace '{workspace_id}' in repository '{repository_id}'.\")\n",
        "    # The workspaceId is passed as a query parameter. [2]\n",
        "    url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/workspaces?workspaceId={workspace_id}\"\n",
        "\n",
        "    # The request body for creating a workspace.\n",
        "    request_body = {\n",
        "        \"name\": workspace_id\n",
        "    }\n",
        "    messages=[]\n",
        "    try:\n",
        "        # Call the REST API helper to execute the POST request. [2]\n",
        "        json_result = rest_api_helper(url, \"POST\", request_body) # Added await\n",
        "\n",
        "        messages.append(f\"Successfully initiated the creation of workspace '{workspace_id}'.\")\n",
        "        #logger.debug(f\"create_workspace json_result: {json_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"create_workspace\",\n",
        "            \"query\": None,\n",
        "            \"created\": True,\n",
        "            \"messages\": messages,\n",
        "            \"results\": json_result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while creating the workspace '{workspace_id}': {e}\"\n",
        "        messages.append(error_message)\n",
        "    #    logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"create_workspace\",\n",
        "            \"query\": None,\n",
        "            \"created\": False,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kiQGecMQ6dtR",
      "metadata": {
        "id": "kiQGecMQ6dtR"
      },
      "source": [
        "# Function to Write Workflow Settings File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3NKwHchnnj60",
      "metadata": {
        "id": "3NKwHchnnj60"
      },
      "outputs": [],
      "source": [
        "def write_workflow_settings_file(repository_id: str, workspace_id: str,dataform_region,project_id) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Writes the 'workflow_settings.yaml' file to a Dataform workspace.\n",
        "\n",
        "    This function creates the 'workflow_settings.yaml' file with a predefined\n",
        "    template, populating it with the current project ID and location. This is\n",
        "    a standard initialization step for Dataform workspaces.\n",
        "\n",
        "    Args:\n",
        "        repository_id (str): The ID of the repository containing the workspace.\n",
        "        workspace_id (str): The ID of the workspace where the file will be written.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and the result of the writeFile operation.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"write_workflow_settings_file\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from the writeFile operation ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "    messages = []\n",
        "\n",
        "    # Define the specific file path and content template within the function\n",
        "    file_path = \"workflow_settings.yaml\"\n",
        "    file_content_template = \"\"\"defaultProject: {project_id}\n",
        "defaultLocation: {location}\n",
        "defaultDataset: dataform\n",
        "defaultAssertionDataset: dataform_assertions\n",
        "dataformCoreVersion: 3.0.16\"\"\"\n",
        "\n",
        "    try:\n",
        "        messages.append(f\"Preparing to write file '{file_path}' to workspace '{workspace_id}'.\")\n",
        "\n",
        "        # Populate the template with the project and location details.\n",
        "        final_file_contents = file_content_template.format(\n",
        "            project_id=project_id,\n",
        "            location=dataform_region\n",
        "        )\n",
        "        messages.append(\"Successfully formatted file content template.\")\n",
        "\n",
        "        # Base64 encode the populated string.\n",
        "        encoded_contents = base64.b64encode(final_file_contents.encode('utf-8')).decode('utf-8')\n",
        "        messages.append(\"Successfully Base64 encoded file contents.\")\n",
        "\n",
        "        write_url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/workspaces/{workspace_id}:writeFile\"\n",
        "\n",
        "        write_request_body = {\n",
        "            \"path\": file_path,\n",
        "            \"contents\": encoded_contents\n",
        "        }\n",
        "\n",
        "        # Execute the writeFile request\n",
        "        write_result = rest_api_helper(write_url, \"POST\", write_request_body) # Added await\n",
        "        messages.append(f\"Successfully wrote file '{file_path}'.\")\n",
        "        #logger.debug(f\"write_workflow_settings_file result: {write_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"write_workflow_settings_file\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": write_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during the write_workflow_settings_file process: {e}\"\n",
        "        messages.append(error_message)\n",
        "        #logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"write_workflow_settings_file\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4G12_6zQ6kHM",
      "metadata": {
        "id": "4G12_6zQ6kHM"
      },
      "source": [
        "# Function to Write Actions Yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6YZzG0QYplt4",
      "metadata": {
        "id": "6YZzG0QYplt4"
      },
      "outputs": [],
      "source": [
        "def write_actions_yaml_file(repository_id: str, workspace_id: str,dataform_region, project_id) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Writes a placeholder 'actions.yaml' file to a Dataform workspace.\n",
        "\n",
        "    This function is specifically designed to create the 'definitions/actions.yaml'\n",
        "    file with the content 'actions: []', which is often required for initializing\n",
        "    BigQuery Pipelines.\n",
        "\n",
        "    Args:\n",
        "        repository_id (str): The ID of the repository containing the workspace.\n",
        "        workspace_id (str): The ID of the workspace where the file will be written.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and the result of the writeFile operation.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"write_actions_yaml_file\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from the writeFile operation ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "    messages = []\n",
        "\n",
        "    # Define the specific file path and content within the function\n",
        "    file_path = \"definitions/actions.yaml\"\n",
        "    file_contents = \"actions: []\"\n",
        "\n",
        "    try:\n",
        "        messages.append(f\"Writing placeholder file '{file_path}' to workspace '{workspace_id}'.\")\n",
        "\n",
        "        write_url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/workspaces/{workspace_id}:writeFile\"\n",
        "\n",
        "        # Base64 encode the predefined file contents\n",
        "        encoded_contents = base64.b64encode(file_contents.encode('utf-8')).decode('utf-8')\n",
        "\n",
        "        write_request_body = {\n",
        "            \"path\": file_path,\n",
        "            \"contents\": encoded_contents\n",
        "        }\n",
        "\n",
        "        # Execute the writeFile request\n",
        "        write_result = rest_api_helper(write_url, \"POST\", write_request_body) # Added await\n",
        "        messages.append(f\"Successfully wrote file '{file_path}'.\")\n",
        "        #logger.debug(f\"write_actions_yaml_file result: {write_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"write_actions_yaml_file\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": write_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during the write_actions_yaml_file process: {e}\"\n",
        "        messages.append(error_message)\n",
        "        #logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"write_actions_yaml_file\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cgStu29r6oZ7",
      "metadata": {
        "id": "cgStu29r6oZ7"
      },
      "source": [
        "#Function to Commit code in Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1l33JW_eqeEf",
      "metadata": {
        "id": "1l33JW_eqeEf"
      },
      "outputs": [],
      "source": [
        "def commit_workspace(repository_id: str, workspace_id: str, author_name: str, author_email: str, commit_message: str) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Commits pending changes in a Dataform workspace.\n",
        "\n",
        "    Args:\n",
        "        repository_id (str): The ID of the repository containing the workspace.\n",
        "        workspace_id (str): The ID of the workspace with pending changes to commit.\n",
        "        author_name (str): The name of the user to be credited as the author of the commit.\n",
        "        author_email (str): The email address of the commit author.\n",
        "        commit_message (str): The message describing the changes being committed.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and results of the operation.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"commit_workspace\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from Dataform ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "    messages = []\n",
        "\n",
        "    # The API endpoint for committing to a workspace.\n",
        "    url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/workspaces/{workspace_id}:commit\"\n",
        "\n",
        "    # The request body containing the author and commit message.\n",
        "    request_body = {\n",
        "        \"author\": {\n",
        "            \"name\": author_name,\n",
        "            \"emailAddress\": author_email\n",
        "        },\n",
        "        \"commitMessage\": commit_message\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        messages.append(f\"Attempting to commit changes to workspace '{workspace_id}' in repository '{repository_id}'.\")\n",
        "\n",
        "        # Call the REST API helper to execute the POST request.\n",
        "        json_result = rest_api_helper(url, \"POST\", request_body) # Added await\n",
        "\n",
        "        messages.append(f\"Successfully committed changes with message: '{commit_message}'.\")\n",
        "        #logger.debug(f\"commit_workspace json_result: {json_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"commit_workspace\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": json_result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while committing to the workspace '{workspace_id}': {e}\"\n",
        "        messages.append(error_message)\n",
        "        #logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"commit_workspace\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gnVPcp4M6t1V",
      "metadata": {
        "id": "gnVPcp4M6t1V"
      },
      "source": [
        "# Function to Compile and Run Your Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6k9rayntiJy",
      "metadata": {
        "id": "a6k9rayntiJy"
      },
      "outputs": [],
      "source": [
        "def compile_and_run_dataform_workflow(repository_id: str, workspace_id: str,dataform_region,project_id,dataform_service_account) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Compiles a Dataform repository from a workspace and then runs the resulting workflow.\n",
        "\n",
        "    This function performs two sequential operations:\n",
        "    1. It creates a compilation result from the specified workspace.\n",
        "    2. It starts a workflow invocation using the successful compilation result.\n",
        "\n",
        "    Args:\n",
        "        repository_id (str): The ID of the Dataform repository to compile and run.\n",
        "        workspace_id (str): The ID of the workspace containing the code to be compiled.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and the final response from the workflow invocation API call.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"compile_and_run_dataform_workflow\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from the workflow invocation ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "    #dataform_service_account = os.getenv(\"AGENT_ENV_DATAFORM_SERVICE_ACCOUNT\")\n",
        "    messages = []\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Compile the repository from the workspace ---\n",
        "        messages.append(f\"Step 1: Compiling repository '{repository_id}' from workspace '{workspace_id}'.\")\n",
        "\n",
        "        compile_url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/compilationResults\"\n",
        "\n",
        "        workspace_full_path = f\"projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/workspaces/{workspace_id}\"\n",
        "\n",
        "        compile_request_body = {\n",
        "            \"workspace\": workspace_full_path\n",
        "        }\n",
        "\n",
        "        compile_result = rest_api_helper(compile_url, \"POST\", compile_request_body) # Added await\n",
        "        compilation_result_name = compile_result.get(\"name\")\n",
        "\n",
        "        # You might want to check the status of the compilation and only start it if it is \"success\"!\n",
        "\n",
        "        if not compilation_result_name:\n",
        "            raise Exception(\"Failed to get compilation result name from the compilation API response.\")\n",
        "\n",
        "        messages.append(f\"Successfully compiled. Compilation result name: {compilation_result_name}\")\n",
        "\n",
        "        # --- Step 2: Run the workflow using the compilation result ---\n",
        "        messages.append(f\"Step 2: Starting workflow execution for compilation '{compilation_result_name}'.\")\n",
        "\n",
        "        invoke_url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_id}/workflowInvocations\"\n",
        "\n",
        "        invoke_request_body = {\n",
        "            \"compilationResult\": compilation_result_name,\n",
        "              \"invocationConfig\": {\n",
        "                \"serviceAccount\": dataform_service_account\n",
        "              }\n",
        "        }\n",
        "\n",
        "        invoke_result = rest_api_helper(invoke_url, \"POST\", invoke_request_body) # Added await\n",
        "\n",
        "        messages.append(\"Successfully initiated workflow invocation.\")\n",
        "        #logger.debug(f\"compile_and_run_dataform_workflow invoke_result: {invoke_result}\") # This comment had (f\"...\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"compile_and_run_dataform_workflow\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"workflow_invocation_id\": invoke_result[\"name\"].rsplit('/', 1)[-1],\n",
        "            \"results\": invoke_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during the compile and run process: {e}\"\n",
        "        messages.append(error_message)\n",
        "        #logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"compile_and_run_dataform_workflow\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qjRKUkpX6y2-",
      "metadata": {
        "id": "qjRKUkpX6y2-"
      },
      "source": [
        "# Function to Call Data Eng Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qGdDW-cpxGGx",
      "metadata": {
        "id": "qGdDW-cpxGGx"
      },
      "outputs": [],
      "source": [
        "def call_bigquery_data_engineering_agent(repository_name: str, workspace_name: str, prompt: str,dataform_region,project_id) -> dict: # Changed to def\n",
        "    \"\"\"\n",
        "    Sends a natural language prompt to the internal BigQuery Data Engineering agent which will generate/update\n",
        "    the Dataform pipeline code based upon the prompt. The BigQuery Data Engineering agent updates the ETL logic\n",
        "    within the specified Dataform workspace.\n",
        "\n",
        "    Args:\n",
        "        repository_name (str): The ID of the Dataform repository to use for the pipeline.\n",
        "        workspace_name (str): The ID of the Dataform workspace within the repository.\n",
        "        prompt (str): The natural language prompt describing the data engineering task to be performed (e.g., \"uppercase the 'city' column\").\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and the response from the API, which may include the generated code and task status.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"call_bigquery_data_engineering_agent\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from Gemini Data Analytics service ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "    messages = []\n",
        "\n",
        "    # The global endpoint for the Gemini Data Analytics service.\n",
        "    # NOTE: Do not take a hard dependency on this REST API call, it will be changing in the future!\n",
        "    url = f\"https://geminidataanalytics.googleapis.com/v1alpha1/projects/{project_id}/locations/global:run\"\n",
        "\n",
        "    # The pipeline_id is the full resource name of the Dataform workspace.\n",
        "    pipeline_id = f\"projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/workspaces/{workspace_name}\"\n",
        "\n",
        "    # The request body containing the pipeline and the user's prompt.\n",
        "    request_body = {\n",
        "      \"parent\": f\"projects/{project_id}/locations/global\",\n",
        "      \"pipeline_id\": pipeline_id,\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"user_message\": {\n",
        "            \"text\": prompt\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        messages.append(f\"Attempting to generate/update data engineering code in workspace '{workspace_name}' for repository '{repository_name}' with prompt: '{prompt}'.\")\n",
        "\n",
        "        # Call the REST API helper to execute the POST request.\n",
        "        json_result = rest_api_helper(url, \"POST\", request_body) # Added await\n",
        "\n",
        "        messages.append(\"Successfully submitted the data engineering task to the Gemini Data Analytics service.\")\n",
        "        #logger.debug(f\"call_bigquery_data_engineering_agent json_result: {json_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"call_bigquery_data_engineering_agent\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": json_result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while calling the BigQuery Data Engineering agent: {e}\"\n",
        "        messages.append(error_message)\n",
        "        #logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"call_bigquery_data_engineering_agent\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sdKI63KQ7mGx",
      "metadata": {
        "id": "sdKI63KQ7mGx"
      },
      "source": [
        "# Function to Call Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cFqNNcn_7k7i",
      "metadata": {
        "id": "cFqNNcn_7k7i"
      },
      "outputs": [],
      "source": [
        "def GeminiLLM(prompt,model_name=\"gemini-2.5-pro\", response_schema=None,\n",
        "                 temperature=1.0, top_p=1.0, top_k=32):\n",
        "\n",
        "\n",
        "  # Load the specified Gemini model.\n",
        "  model = GenerativeModel(model_name)\n",
        "\n",
        "  # Configure the generation parameters.\n",
        "  generation_config = GenerationConfig(\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k,\n",
        "      max_output_tokens=8192,\n",
        "      response_mime_type=\"application/json\" if response_schema else \"text/plain\",\n",
        "      response_schema=response_schema\n",
        "  )\n",
        "\n",
        "  # Generate the content.\n",
        "  response = model.generate_content(\n",
        "      [prompt],\n",
        "      generation_config=generation_config,\n",
        "  )\n",
        "\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PbVvM8tT7Gy3",
      "metadata": {
        "id": "PbVvM8tT7Gy3"
      },
      "source": [
        "# Now we are Gonna Call all the functions to generate Pipeline and kickoff execution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q0YmbI8U7Pzq",
      "metadata": {
        "id": "q0YmbI8U7Pzq"
      },
      "source": [
        "# Create BigQuery Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wWzw7jfx5npr",
      "metadata": {
        "id": "wWzw7jfx5npr"
      },
      "outputs": [],
      "source": [
        "create_bigquery_pipeline(pipeline_name, display_name,dataform_region, service_account, project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fxkiYUcUTMmc",
      "metadata": {
        "id": "fxkiYUcUTMmc"
      },
      "source": [
        "#Create Dataform Pipeline - Choose either BigQuery Pipeline or Dataform Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WLTrg8qQTVEX",
      "metadata": {
        "id": "WLTrg8qQTVEX"
      },
      "outputs": [],
      "source": [
        "create_dataform_pipeline(pipeline_name, display_name,dataform_region, service_account, project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D_ajJ8uj7UEW",
      "metadata": {
        "id": "D_ajJ8uj7UEW"
      },
      "source": [
        "# Create Workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yhEAEy3kf6pI",
      "metadata": {
        "id": "yhEAEy3kf6pI"
      },
      "outputs": [],
      "source": [
        "create_workspace(pipeline_name,workspace_id,dataform_region,project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4S7hYfID7Y98",
      "metadata": {
        "id": "4S7hYfID7Y98"
      },
      "source": [
        "# Create Workflow Settings File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-qTUquBZn5m5",
      "metadata": {
        "id": "-qTUquBZn5m5"
      },
      "outputs": [],
      "source": [
        "write_workflow_settings_file(pipeline_name,workspace_id,dataform_region,project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vUwJzRKE9Aaq",
      "metadata": {
        "id": "vUwJzRKE9Aaq"
      },
      "source": [
        "# Commit Workspace Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2gScUMcK9Lxw",
      "metadata": {
        "id": "2gScUMcK9Lxw"
      },
      "outputs": [],
      "source": [
        "commit_workspace(pipeline_name,workspace_id,\"n\", \"n@example.com\",\"BQ pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAQ-O_prG-6U",
      "metadata": {
        "id": "xAQ-O_prG-6U"
      },
      "source": [
        "# Construct prompt to get instructions for Data Eng Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j9ELmmJOaPtG",
      "metadata": {
        "id": "j9ELmmJOaPtG"
      },
      "outputs": [],
      "source": [
        "prompt_code = f\"\"\"I'm trying to convert below pyspark code into SQL pipeline using Data Engineering Agent, Data eng Agent takes\n",
        "            instructions in natural Language and gives me SQL pipeline, take the below code and generate step by step\n",
        "            explanation of the code which I can feed into Data eng Agent, ignore any steps where you are connecting to spark, give me steps in raw string\n",
        "            example:\n",
        "            Step 1: Read data from BQ table A, Table B, Table C\n",
        "            Step 2: join tables\n",
        "            Step 3: write data into a new table Table D\n",
        "\n",
        "\n",
        "        Pyspark Code: {sample_pyspark_code}\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YaBto_7k8wC_",
      "metadata": {
        "id": "YaBto_7k8wC_"
      },
      "outputs": [],
      "source": [
        "print(prompt_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yo9zED0PGNrd",
      "metadata": {
        "id": "Yo9zED0PGNrd"
      },
      "source": [
        "# Call Gemini Pro to generate instructions for Data Eng agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U7I5NnKhaUFU",
      "metadata": {
        "id": "U7I5NnKhaUFU"
      },
      "outputs": [],
      "source": [
        "# Use LLM to generate data\n",
        "llm_response_raw = GeminiLLM(prompt_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9IpCk0UI0cOu",
      "metadata": {
        "id": "9IpCk0UI0cOu"
      },
      "outputs": [],
      "source": [
        "print(llm_response_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r5Baoql19Rmu",
      "metadata": {
        "id": "r5Baoql19Rmu"
      },
      "source": [
        "# Call Data Eng Agent to generate SQL Pipeline Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2uHvQkzebr",
      "metadata": {
        "id": "5e2uHvQkzebr"
      },
      "outputs": [],
      "source": [
        "call_bigquery_data_engineering_agent(pipeline_name,workspace_id,llm_response_raw, dataform_region,project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LylisleX9c-3",
      "metadata": {
        "id": "LylisleX9c-3"
      },
      "source": [
        "# Commit Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150pRm8azhBm",
      "metadata": {
        "id": "150pRm8azhBm"
      },
      "outputs": [],
      "source": [
        "commit_workspace(pipeline_name,workspace_id,\"n\", \"n@example.com\",\"BQ dataengpipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nNE5Lq_w9g9R",
      "metadata": {
        "id": "nNE5Lq_w9g9R"
      },
      "source": [
        "# Write Actions File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6M0F9TKhpyQA",
      "metadata": {
        "id": "6M0F9TKhpyQA"
      },
      "outputs": [],
      "source": [
        "write_actions_yaml_file(pipeline_name,workspace_id,dataform_region,project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fmVeOJQ9mOj",
      "metadata": {
        "id": "0fmVeOJQ9mOj"
      },
      "source": [
        "# Commit Workspace Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rUWLf1BUxgif",
      "metadata": {
        "id": "rUWLf1BUxgif"
      },
      "outputs": [],
      "source": [
        "commit_workspace(pipeline_name,workspace_id,\"n\", \"n@example.com\",\"BQ actionsfile\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x6_WrtAJ9qrd",
      "metadata": {
        "id": "x6_WrtAJ9qrd"
      },
      "source": [
        "# Compile and Run the Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMMbdsMduggQ",
      "metadata": {
        "id": "pMMbdsMduggQ"
      },
      "outputs": [],
      "source": [
        "compile_and_run_dataform_workflow(pipeline_name,workspace_id,dataform_region,project_id,service_account)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sr_R4QR3HIO8",
      "metadata": {
        "id": "sr_R4QR3HIO8"
      },
      "source": [
        "# Look at Dataform repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tkANEtvgK9Bt",
      "metadata": {
        "id": "tkANEtvgK9Bt"
      },
      "outputs": [],
      "source": [
        "# You can view BigQuery pipelines in the BigQuery UI or the Dataform UI (click link below)\n",
        "# The below line will open a Dataform repo\n",
        "\n",
        "print(f\"\"\"https://console.cloud.google.com/bigquery/dataform/locations/us-central1/repositories/{pipeline_name}/workspaces/{workspace_id}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Z2vW1yc92YT",
      "metadata": {
        "id": "8Z2vW1yc92YT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Q43JtzGbvfc2",
        "FIMkWrSD8Ymr"
      ],
      "name": "Demo-Data-Engineering-Agent-Spark-Conversion",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
