{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eL2q138uw8S"
      },
      "source": [
        "### <font color='#4285f4'>Overview</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS2SRVMHuw8U"
      },
      "source": [
        "Overview: Shows how you can call the Data Engineering agent via the REST API to correct a broken pipeline\n",
        "\n",
        "Process Flow:\n",
        "1. Create both good and bad data\n",
        "2. Runs a data quality scan\n",
        "3. Gets the broken rules / data\n",
        "4. Creates a prompt\n",
        "5. Sends the prompt to the Data Engineering Agent\n",
        "6. Runs the workflow/pipeline\n",
        "\n",
        "\n",
        "Cost:\n",
        "* Approximate cost: Less than $1\n",
        "\n",
        "Author:\n",
        "* Adam Paternostro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Architecture Diagram\n",
        "from IPython.display import Image\n",
        "Image(url='https://storage.googleapis.com/data-analytics-golden-demo/colab-diagrams/autonomous-pipeline-repair.png', width=1200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_hBijiyuw8V"
      },
      "source": [
        "### <font color='#4285f4'>Video Walkthrough</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZdJRRnluw8V"
      },
      "source": [
        "[Video](https://storage.googleapis.com/data-analytics-golden-demo/colab-videos/Demo-Agent-Self-Healing-Pipeline.mp4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKy_JudOuw8V"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"800\" height=\"600\" controls>\n",
        "  <source src=\"https://storage.googleapis.com/data-analytics-golden-demo/colab-videos/Demo-Agent-Self-Healing-Pipeline.mp4\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "</video>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMsUvoF4BP7Y"
      },
      "source": [
        "### <font color='#4285f4'>License</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQgQkbOvj55d"
      },
      "source": [
        "```\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m65vp54BUFRi"
      },
      "source": [
        "### <font color='#4285f4'>Pip installs</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MaWM6H5i6rX"
      },
      "outputs": [],
      "source": [
        "# PIP Installs (if necessary)\n",
        "import sys\n",
        "\n",
        "# !{sys.executable} -m pip install REPLACE-ME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmyL-Rg4Dr_f"
      },
      "source": [
        "### <font color='#4285f4'>Initialize</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOYsEVSXp6IP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import HTML\n",
        "import IPython.display\n",
        "import google.auth\n",
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "import base64\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import base64\n",
        "import random\n",
        "\n",
        "import logging\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, before_sleep_log, retry_if_exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMlHl3bnkFPZ"
      },
      "outputs": [],
      "source": [
        "# Set these (run this cell to verify the output)\n",
        "\n",
        "bigquery_location = \"${bigquery_non_multi_region}\"\n",
        "dataplex_region = \"${dataplex_region}\"\n",
        "dataform_region = \"${dataform_region}\"\n",
        "repository_name = \"agentic-beans-repo\"\n",
        "workspace_name_original = \"telemetry-coffee-machine-original\"\n",
        "workspace_name_auto = \"telemetry-coffee-machine-auto\"\n",
        "location = \"${location}\" # for Gemini\n",
        "gcp_account_name = \"${gcp_account_name}\"\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Get the current date and time\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "# Format the date and time as desired\n",
        "formatted_date = now.strftime(\"%Y-%m-%d-%H-%M\")\n",
        "\n",
        "# Get some values using gcloud\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "user = !(gcloud auth list --filter=status:ACTIVE --format=\"value(account)\")\n",
        "\n",
        "if len(user) != 1:\n",
        "  raise RuntimeError(f\"user is not set: {user}\")\n",
        "user = user[0]\n",
        "\n",
        "print(f\"project_id = {project_id}\")\n",
        "print(f\"user = {user}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6m_wGrK0YG"
      },
      "source": [
        "### <font color='#4285f4'>Helper Methods</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOjdSP1kN9T"
      },
      "source": [
        "#### rest_api_helper\n",
        "Calls the Google Cloud REST API using the current users credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40wlwnY4kM11"
      },
      "outputs": [],
      "source": [
        "def rest_api_helper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import google.auth.transport.requests\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error rest_api_helper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W8avEv6uw8X"
      },
      "source": [
        "#### RetryCondition (for retrying LLM calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgJe5mQ6uw8X"
      },
      "outputs": [],
      "source": [
        "def RetryCondition(error):\n",
        "  error_string = str(error)\n",
        "  print(error_string)\n",
        "\n",
        "  retry_errors = [\n",
        "      \"RESOURCE_EXHAUSTED\",\n",
        "      \"No content in candidate\",\n",
        "      # Add more error messages here as needed\n",
        "  ]\n",
        "\n",
        "  for retry_error in retry_errors:\n",
        "    if retry_error in error_string:\n",
        "      print(\"Retrying...\")\n",
        "      return True\n",
        "\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOFTk6sj1YIV"
      },
      "source": [
        "#### Gemini LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHit3Hh-1ZAW"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_exponential(multiplier=1, min=1, max=60), stop=stop_after_attempt(10), retry=retry_if_exception(RetryCondition), before_sleep=before_sleep_log(logging.getLogger(), logging.INFO))\n",
        "def GeminiLLM(prompt, model = \"gemini-2.0-flash\", response_schema = None,\n",
        "                 temperature = 1, topP = 1, topK = 32):\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#supported_models\n",
        "\n",
        "  llm_response = None\n",
        "  if temperature < 0:\n",
        "    temperature = 0\n",
        "\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request() # required to acess access token\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "      \"Content-Type\" : \"application/json\",\n",
        "      \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference\n",
        "  url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model}:generateContent\"\n",
        "\n",
        "  generation_config = {\n",
        "    \"temperature\": temperature,\n",
        "    \"topP\": topP,\n",
        "    \"maxOutputTokens\": 8192,\n",
        "    \"candidateCount\": 1,\n",
        "    \"responseMimeType\": \"application/json\",\n",
        "  }\n",
        "\n",
        "  # Add inthe response schema for when it is provided\n",
        "  if response_schema is not None:\n",
        "    generation_config[\"responseSchema\"] = response_schema\n",
        "\n",
        "  if model == \"gemini-2.0-flash\":\n",
        "    generation_config[\"topK\"] = topK\n",
        "\n",
        "  payload = {\n",
        "    \"contents\": {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": {\n",
        "          \"text\": prompt\n",
        "      },\n",
        "    },\n",
        "    \"generation_config\": {\n",
        "      **generation_config\n",
        "    },\n",
        "    \"safety_settings\": {\n",
        "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    try:\n",
        "      json_response = json.loads(response.content)\n",
        "    except Exception as error:\n",
        "      raise RuntimeError(f\"An error occurred parsing the JSON: {error}\")\n",
        "\n",
        "    if \"candidates\" in json_response:\n",
        "      candidates = json_response[\"candidates\"]\n",
        "      if len(candidates) > 0:\n",
        "        candidate = candidates[0]\n",
        "        if \"content\" in candidate:\n",
        "          content = candidate[\"content\"]\n",
        "          if \"parts\" in content:\n",
        "            parts = content[\"parts\"]\n",
        "            if len(parts):\n",
        "              part = parts[0]\n",
        "              if \"text\" in part:\n",
        "                text = part[\"text\"]\n",
        "                llm_response = text\n",
        "              else:\n",
        "                raise RuntimeError(\"No text in part: {response.content}\")\n",
        "            else:\n",
        "              raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "          else:\n",
        "            raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "        else:\n",
        "          raise RuntimeError(\"No content in candidate: {response.content}\")\n",
        "      else:\n",
        "        raise RuntimeError(\"No candidates: {response.content}\")\n",
        "    else:\n",
        "      raise RuntimeError(\"No candidates: {response.content}\")\n",
        "\n",
        "    # Remove some typically response characters (if asking for a JSON reply)\n",
        "    llm_response = llm_response.replace(\"```json\",\"\")\n",
        "    llm_response = llm_response.replace(\"```\",\"\")\n",
        "    llm_response = llm_response.replace(\"\\n\",\"\")\n",
        "\n",
        "    return llm_response\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(f\"Error with prompt:'{prompt}'  Status:'{response.status_code}' Text:'{response.text}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWXSCd5VCPjf"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_exponential(multiplier=1, min=1, max=60), stop=stop_after_attempt(10), retry=retry_if_exception(RetryCondition), before_sleep=before_sleep_log(logging.getLogger(), logging.INFO))\n",
        "def GeminiLLM_VerifyImage(prompt, imageBase64, model = \"gemini-2.0-flash\", response_schema = None,\n",
        "                 temperature = 1, topP = 1, topK = 32):\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#supported_models\n",
        "\n",
        "  llm_response = None\n",
        "  if temperature < 0:\n",
        "    temperature = 0\n",
        "\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request() # required to acess access token\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "      \"Content-Type\" : \"application/json\",\n",
        "      \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference\n",
        "  url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model}:generateContent\"\n",
        "\n",
        "  generation_config = {\n",
        "    \"temperature\": temperature,\n",
        "    \"topP\": topP,\n",
        "    \"maxOutputTokens\": 8192,\n",
        "    \"candidateCount\": 1,\n",
        "    \"responseMimeType\": \"application/json\",\n",
        "  }\n",
        "\n",
        "  # Add inthe response schema for when it is provided\n",
        "  if response_schema is not None:\n",
        "    generation_config[\"responseSchema\"] = response_schema\n",
        "\n",
        "  if model == \"gemini-2.0-flash\":\n",
        "    generation_config[\"topK\"] = topK\n",
        "\n",
        "  payload = {\n",
        "    \"contents\": {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "          { \"text\": prompt },\n",
        "          { \"inlineData\": {  \"mimeType\": \"image/png\", \"data\": f\"{imageBase64}\" } }\n",
        "        ]\n",
        "    },\n",
        "    \"generation_config\": {\n",
        "      **generation_config\n",
        "    },\n",
        "    \"safety_settings\": {\n",
        "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    try:\n",
        "      json_response = json.loads(response.content)\n",
        "    except Exception as error:\n",
        "      raise RuntimeError(f\"An error occurred parsing the JSON: {error}\")\n",
        "\n",
        "    if \"candidates\" in json_response:\n",
        "      candidates = json_response[\"candidates\"]\n",
        "      if len(candidates) > 0:\n",
        "        candidate = candidates[0]\n",
        "        if \"content\" in candidate:\n",
        "          content = candidate[\"content\"]\n",
        "          if \"parts\" in content:\n",
        "            parts = content[\"parts\"]\n",
        "            if len(parts):\n",
        "              part = parts[0]\n",
        "              if \"text\" in part:\n",
        "                text = part[\"text\"]\n",
        "                llm_response = text\n",
        "              else:\n",
        "                raise RuntimeError(\"No text in part: {response.content}\")\n",
        "            else:\n",
        "              raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "          else:\n",
        "            raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "        else:\n",
        "          raise RuntimeError(\"No content in candidate: {response.content}\")\n",
        "      else:\n",
        "        raise RuntimeError(\"No candidates: {response.content}\")\n",
        "    else:\n",
        "      raise RuntimeError(\"No candidates: {response.content}\")\n",
        "\n",
        "    # Remove some typically response characters (if asking for a JSON reply)\n",
        "    llm_response = llm_response.replace(\"```json\",\"\")\n",
        "    llm_response = llm_response.replace(\"```\",\"\")\n",
        "    llm_response = llm_response.replace(\"\\n\",\"\")\n",
        "\n",
        "    return llm_response\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(f\"Error with prompt:'{prompt}'  Status:'{response.status_code}' Text:'{response.text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI-KJELZ1jgt"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmnCwYvA1kZv"
      },
      "outputs": [],
      "source": [
        "def RunQuery(sql):\n",
        "  import time\n",
        "  from google.cloud import bigquery\n",
        "  client = bigquery.Client()\n",
        "\n",
        "  if (sql.startswith(\"SELECT\") or sql.startswith(\"WITH\")):\n",
        "      df_result = client.query(sql).to_dataframe()\n",
        "      return df_result\n",
        "  else:\n",
        "    job_config = bigquery.QueryJobConfig(priority=bigquery.QueryPriority.INTERACTIVE)\n",
        "    query_job = client.query(sql, job_config=job_config)\n",
        "\n",
        "    # Check on the progress by getting the job's updated state.\n",
        "    query_job = client.get_job(\n",
        "        query_job.job_id, location=query_job.location\n",
        "    )\n",
        "    print(\"Job {} is currently in state {} with error result of {}\".format(query_job.job_id, query_job.state, query_job.error_result))\n",
        "\n",
        "    while query_job.state != \"DONE\":\n",
        "      time.sleep(2)\n",
        "      query_job = client.get_job(\n",
        "          query_job.job_id, location=query_job.location\n",
        "          )\n",
        "      print(\"Job {} is currently in state {} with error result of {}\".format(query_job.job_id, query_job.state, query_job.error_result))\n",
        "\n",
        "    if query_job.error_result == None:\n",
        "      return True\n",
        "    else:\n",
        "      raise Exception(query_job.error_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWpDCqGFwO13"
      },
      "source": [
        "### <font color='#4285f4'>Data Quality Scan - Helper Methods</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlaNsfhiwPuI"
      },
      "outputs": [],
      "source": [
        "def getStateDataQualityScan(project_id, dataplex_region, data_quality_scan_job_name):\n",
        "  \"\"\"Runs the data quality scan job and monitors until it completes\"\"\"\n",
        "\n",
        "  # Gets the \"state\" of a scan\n",
        "  url = f\"https://dataplex.googleapis.com/v1/{data_quality_scan_job_name}\"\n",
        "  json_result = rest_api_helper(url, \"GET\", None)\n",
        "  return json_result[\"state\"]\n",
        "  #== \"STATE_UNSPECIFIED\" or json_result[\"state\"] == \"RUNNING\" or json_result[\"state\"] == \"PENDING\":\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VXbom7VwQbe"
      },
      "outputs": [],
      "source": [
        "def startDataQualityScan(project_id, dataplex_region, data_quality_scan_name):\n",
        "  \"\"\"Runs the data profile scan job and monitors until it completes\"\"\"\n",
        "\n",
        "  # Start a scan\n",
        "  # https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.dataScans/run\n",
        "  print(\"Running Data Quality Scan\")\n",
        "\n",
        "  url = f\"https://dataplex.googleapis.com/v1/projects/{project_id}/locations/{dataplex_region}/dataScans/{data_quality_scan_name}:run\"\n",
        "\n",
        "\n",
        "  request_body = { }\n",
        "\n",
        "  json_result = rest_api_helper(url, \"POST\", request_body)\n",
        "  job_name = json_result[\"job\"][\"name\"]\n",
        "  job_state = json_result[\"job\"][\"state\"]\n",
        "  print(f\"Document Data Scan Run created: {job_name} - State: {job_state}\")\n",
        "\n",
        "  return job_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI3pXdMZwQYz"
      },
      "outputs": [],
      "source": [
        "def updateBigQueryTableDataplexLabels(project_id, dataplex_region, dataplex_asset_type, dataplex_asset_scan_name, bigquery_dataset_name, bigquery_table_name):\n",
        "  \"\"\"Sets the labels on the BigQuery table so users can see the data profile in the Console.\"\"\"\n",
        "\n",
        "  # Patch BigQuery\n",
        "  # https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.dataScans/create\n",
        "  print(\"Patching BigQuery Dataplex Labels\")\n",
        "\n",
        "  url = f\"https://bigquery.googleapis.com/bigquery/v2/projects/{project_id}/datasets/{bigquery_dataset_name}/tables/{bigquery_table_name}\"\n",
        "\n",
        "  request_body = {}\n",
        "  if dataplex_asset_type == \"DATA-PROFILE-SCAN\":\n",
        "    request_body = {\n",
        "        \"labels\" : {\n",
        "            \"dataplex-dp-published-project\"  : project_id,\n",
        "            \"dataplex-dp-published-location\" : dataplex_region,\n",
        "            \"dataplex-dp-published-scan\"     : dataplex_asset_scan_name,\n",
        "            }\n",
        "        }\n",
        "  elif dataplex_asset_type == \"DATA-INSIGHTS-SCAN\":\n",
        "     request_body = {\n",
        "        \"labels\" : {\n",
        "            \"dataplex-data-documentation-project\"  : project_id,\n",
        "            \"dataplex-data-documentation-location\" : dataplex_region,\n",
        "            \"dataplex-data-documentation-scan\"     : dataplex_asset_scan_name,\n",
        "            }\n",
        "        }\n",
        "  elif dataplex_asset_type == \"DATA-QUALITY-SCAN\":\n",
        "     request_body = {\n",
        "        \"labels\" : {\n",
        "\n",
        "            \"dataplex-dq-published-project\"  : project_id,\n",
        "            \"dataplex-dq-published-location\" : dataplex_region,\n",
        "            \"dataplex-dq-published-scan\"     : dataplex_asset_scan_name,\n",
        "            }\n",
        "        }\n",
        "  else:\n",
        "    raise Exception(f\"Unknown dataplex_asset_type of {dataplex_asset_type}\")\n",
        "\n",
        "  json_result = rest_api_helper(url, \"PATCH\", request_body)\n",
        "  print(json_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOTMPIS3LX-"
      },
      "source": [
        "### <font color='#4285f4'>Dataform - Helper Methods</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To-b0nqH3Oh2"
      },
      "outputs": [],
      "source": [
        "def compile_and_run_dataform_workflow(repository_name: str, workspace_name: str) -> dict:\n",
        "    \"\"\"\n",
        "    Compiles a Dataform repository from a workspace and then runs the resulting workflow.\n",
        "\n",
        "    This function performs two sequential operations:\n",
        "    1. It creates a compilation result from the specified workspace.\n",
        "    2. It starts a workflow invocation using the successful compilation result.\n",
        "\n",
        "    Args:\n",
        "        repository_name (str): The ID of the Dataform repository to compile and run.\n",
        "        workspace_name (str): The ID of the workspace containing the code to be compiled.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and the final response from the workflow invocation API call.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"compile_and_run_dataform_workflow\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": { ... API response from the workflow invocation ... }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\", \"us-central1\")\n",
        "    messages = []\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Compile the repository from the workspace ---\n",
        "        messages.append(f\"Step 1: Compiling repository '{repository_name}' from workspace '{workspace_name}'.\")\n",
        "\n",
        "        compile_url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/compilationResults\"\n",
        "\n",
        "        workspace_full_path = f\"projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/workspaces/{workspace_name}\"\n",
        "\n",
        "        compile_request_body = {\n",
        "            \"workspace\": workspace_full_path\n",
        "        }\n",
        "\n",
        "        compile_result = rest_api_helper(compile_url, \"POST\", compile_request_body)\n",
        "        compilation_result_name = compile_result.get(\"name\")\n",
        "\n",
        "        # You might want to check the status of the compilation and only start it if it is \"success\"!\n",
        "\n",
        "        if not compilation_result_name:\n",
        "            raise Exception(\"Failed to get compilation result name from the compilation API response.\")\n",
        "\n",
        "        messages.append(f\"Successfully compiled. Compilation result name: {compilation_result_name}\")\n",
        "\n",
        "        # --- Step 2: Run the workflow using the compilation result ---\n",
        "        messages.append(f\"Step 2: Starting workflow execution for compilation '{compilation_result_name}'.\")\n",
        "\n",
        "        invoke_url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/workflowInvocations\"\n",
        "\n",
        "        invoke_request_body = {\n",
        "            \"compilationResult\": compilation_result_name,\n",
        "              \"invocationConfig\": {\n",
        "                \"serviceAccount\": f\"bigquery-pipeline-sa@{project_id}.iam.gserviceaccount.com\"\n",
        "              }\n",
        "        }\n",
        "\n",
        "        invoke_result = rest_api_helper(invoke_url, \"POST\", invoke_request_body)\n",
        "\n",
        "        messages.append(\"Successfully initiated workflow invocation.\")\n",
        "        #(f\"compile_and_run_dataform_workflow invoke_result: {invoke_result}\")\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"compile_and_run_dataform_workflow\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"workflow_invocation_id\": invoke_result[\"name\"].rsplit('/', 1)[-1],\n",
        "            \"results\": invoke_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred during the compile and run process: {e}\"\n",
        "        messages.append(error_message)\n",
        "        logger.debug(error_message)\n",
        "        return {\n",
        "            \"status\": \"failed\",\n",
        "            \"tool_name\": \"compile_and_run_dataform_workflow\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": None\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG01QUn_3OUl"
      },
      "outputs": [],
      "source": [
        "def get_worflow_invocation_status(repository_name: str, workflow_invocation_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Checks on the execution status of a workflow.\n",
        "\n",
        "    Args:\n",
        "        repository_name (str): The ID of the Dataform repository to compile and run.\n",
        "        workflow_invocation_id (str): The ID (guid) of workflow invocations id executing a pipeline.  It will return\n",
        "            a workflow_invocation_id value which can be used to check on the execution status.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the status and a boolean result.\n",
        "        {\n",
        "            \"status\": \"success\" or \"failed\",\n",
        "            \"tool_name\": \"get_worflow_invocation_status\",\n",
        "            \"query\": None,\n",
        "            \"messages\": [\"List of messages during processing\"],\n",
        "            \"results\": {\n",
        "                \"name\": \"projects/{project-id}/locations/us-central1/repositories/adam-agent-10-workflow/workflowInvocations/1752598992-06e003bc-aad3-477f-b761-02629a4d554f\",\n",
        "                \"compilationResult\": \"projects/{project-number}/locations/us-central1/repositories/adam-agent-10-workflow/compilationResults/d4a2fa7c-c546-428a-814a-b8eece65a559\",\n",
        "                \"state\": \"SUCCEEDED\",\n",
        "                \"invocationTiming\": {\n",
        "                    \"startTime\": \"2025-07-15T17:03:12.313196Z\",\n",
        "                    \"endTime\": \"2025-07-15T17:03:17.650637343Z\"\n",
        "                },\n",
        "                \"resolvedCompilationResult\": \"projects/{project-number}/locations/us-central1/repositories/adam-agent-10-workflow/compilationResults/d4a2fa7c-c546-428a-814a-b8eece65a559\",\n",
        "                \"internalMetadata\": \"{\\\"db_metadata_insert_time\\\":\\\"2025-07-15T17:03:12.321373Z\\\",\\\"quota_server_enabled\\\":true,\\\"service_account\\\":\\\"service-{project-number}@gcp-sa-dataform.iam.gserviceaccount.com\\\"}\"\n",
        "                }\n",
        "        }\n",
        "    \"\"\"\n",
        "    #project_id = os.getenv(\"AGENT_ENV_PROJECT_ID\")\n",
        "    #dataform_region = os.getenv(\"AGENT_ENV_DATAFORM_REGION\")\n",
        "    messages = []\n",
        "\n",
        "    # The URL to list all repositories in the specified project and region. [1]\n",
        "    url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/workflowInvocations/{workflow_invocation_id}\"\n",
        "    logger.debug(url)\n",
        "\n",
        "    try:\n",
        "        messages.append(f\"Checkin on workflow invoation status with workflow_invocation_id: '{workflow_invocation_id}'.\")\n",
        "        # Call the REST API to get the list of all existing repositories. [1]\n",
        "        json_result = rest_api_helper(url, \"GET\", None)\n",
        "        logger.debug(json_result)\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"tool_name\": \"get_worflow_invocation_status\",\n",
        "            \"query\": None,\n",
        "            \"messages\": messages,\n",
        "            \"results\": json_result\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.debug(e)\n",
        "        # Check if the string representation of the error contains '404'\n",
        "        if '404' in str(e):\n",
        "            messages.append(f\"Workflow Invocation not found for '{workflow_invocation_id}'. This is an expected outcome.\")\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"tool_name\": \"get_worflow_invocation_status\",\n",
        "                \"query\": None,\n",
        "                \"messages\": messages,\n",
        "                \"results\": { \"state\" : \"NOT_FOUND\" }\n",
        "            }\n",
        "        else:\n",
        "            # Handle all other errors as failures\n",
        "            error_message = f\"An unexpected error occurred while checking for existence of file: {e}\"\n",
        "            messages.append(error_message)\n",
        "            return {\n",
        "                \"status\": \"failed\",\n",
        "                \"tool_name\": \"get_worflow_invocation_status\",\n",
        "                \"query\": None,\n",
        "                \"messages\": messages,\n",
        "                \"results\": None\n",
        "            }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c51M89g0Ejmz"
      },
      "source": [
        "### <font color='#4285f4'>MAIN CODE - Using Data Quality to repair a Broken Pipeline</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO8guzdOvTlQ"
      },
      "source": [
        "#### <font color='Green'>Show the Pipeline **Working**<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKsL3F3CvwTK"
      },
      "source": [
        "#### Clean and populate the staging load table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4t2xDvvvkBo"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "TRUNCATE TABLE `agentic_beans_raw_staging_load.telemetry_coffee_machine`;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1rkr0wvvz6a"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "INSERT INTO `agentic_beans_raw_staging_load.telemetry_coffee_machine`\n",
        "(telemetry_coffee_machine_id, telemetry_load_id, machine_id, truck_id, telemetry_timestamp, boiler_temperature_celsius, brew_pressure_bar, water_flow_rate_ml_per_sec, grinder_motor_rpm, grinder_motor_torque_nm, water_reservoir_level_percent, bean_hopper_level_grams, total_brew_cycles_counter, last_error_code, last_error_description, power_consumption_watts, cleaning_cycle_status)\n",
        "SELECT CAST(GENERATE_UUID() AS STRING) AS telemetry_coffee_machine_id,\n",
        "       FORMAT_TIMESTAMP('%Y%m%d%H%M%S', CURRENT_TIMESTAMP()) || '_machine_batch' telemetry_load_id,\n",
        "       CAST(machine_id AS STRING) AS machine_id,\n",
        "       CAST(truck_id AS STRING) AS truck_id,\n",
        "       FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S %Z', CURRENT_TIMESTAMP()) AS telemetry_timestamp,\n",
        "       CAST(boiler_temperature_celsius AS STRING) AS boiler_temperature_celsius,\n",
        "       CAST(brew_pressure_bar AS STRING) AS brew_pressure_bar,\n",
        "       CAST(water_flow_rate_ml_per_sec AS STRING) AS water_flow_rate_ml_per_sec,\n",
        "       CAST(grinder_motor_rpm AS STRING) AS grinder_motor_rpm,\n",
        "       CAST(grinder_motor_torque_nm AS STRING) AS grinder_motor_torque_nm,\n",
        "       CAST(water_reservoir_level_percent AS STRING) AS water_reservoir_level_percent,\n",
        "       CAST(bean_hopper_level_grams AS STRING) AS bean_hopper_level_grams,\n",
        "       CAST(total_brew_cycles_counter AS STRING) AS total_brew_cycles_counter,\n",
        "       CAST(last_error_code AS STRING) AS last_error_code,\n",
        "       CAST(last_error_description AS STRING) AS last_error_description,\n",
        "       CAST(power_consumption_watts AS STRING) AS power_consumption_watts,\n",
        "       CAST(cleaning_cycle_status AS STRING) AS cleaning_cycle_status\n",
        "\n",
        " FROM `agentic_beans_raw.telemetry_coffee_machine` AS telemetry_coffee_machine\n",
        "WHERE telemetry_timestamp = (SELECT max(telemetry_timestamp) FROM `agentic_beans_raw.telemetry_coffee_machine`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_kklVPwvW9D"
      },
      "source": [
        "#### Run the Data Quality Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si9Uiob-v7gn"
      },
      "outputs": [],
      "source": [
        "data_quality_scan_name = \"telemetry-coffee-machine-staging-load-dq\"\n",
        "data_quality_scan_job_name = startDataQualityScan(project_id, dataplex_region, data_quality_scan_name)\n",
        "print(f\"Data Quality Scan Started: {data_quality_scan_job_name}\")\n",
        "time.sleep(10)\n",
        "data_quality_scan_state = getStateDataQualityScan(project_id, dataplex_region, data_quality_scan_job_name)\n",
        "\n",
        "while data_quality_scan_state == \"PENDING\" or \\\n",
        "      data_quality_scan_state == \"STATE_UNSPECIFIED\" or \\\n",
        "      data_quality_scan_state == \"RUNNING\" or \\\n",
        "      data_quality_scan_state == \"CANCELING\":\n",
        "\n",
        "  print(f\"Data Quality Scan State: {data_quality_scan_state}\")\n",
        "  time.sleep(10)\n",
        "  data_quality_scan_state = getStateDataQualityScan(project_id, dataplex_region, data_quality_scan_job_name)\n",
        "\n",
        "if data_quality_scan_state == \"SUCCEEDED\":\n",
        "  print(f\"Data Quality Scan Completed Successfully\")\n",
        "else:\n",
        "  print(f\"Data Quality Scan Failed with status of: {data_quality_scan_state}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azjrClquBBhN"
      },
      "source": [
        "#### Check the Data Quality Job Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpK3LdleBDuD"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "SELECT data_quality_job_id\n",
        "  FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        " WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq'\n",
        "   AND job_start_time = (SELECT MAX(job_start_time)\n",
        "                           FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                          WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq')\n",
        " LIMIT 1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LVGdx9CBNvm"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "SELECT COUNT(*) AS FailedCount\n",
        "FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "WHERE data_quality_job_id       = (SELECT data_quality_job_id\n",
        "                                     FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                                    WHERE data_quality_scan.data_scan_id = 'ttelemetry-coffee-machine-staging-load-dq'\n",
        "                                      AND job_start_time = (SELECT MAX(job_start_time)\n",
        "                                                              FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                                                             WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq')\n",
        "                                    LIMIT 1)\n",
        "AND rule_passed = FALSE;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is2FTCBjvXrZ"
      },
      "source": [
        "#### Run the Dataform Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg--TQqRyuUN"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_original = compile_and_run_dataform_workflow(repository_name, workspace_name_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAUFZmsQ2GJS"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcl65qGByuIv"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_original_id=  workflow_invocation_original[\"results\"][\"name\"].split('/')[-1]\n",
        "time.sleep(30)  # should only take about 20 seconds to complete\n",
        "get_worflow_invocation_status(repository_name, workflow_invocation_original_id)[\"results\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFuGQQ-fvYIF"
      },
      "source": [
        "#### <font color='red'>Show the Pipeline **BROKEN**<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TQjrzV1_o0N"
      },
      "source": [
        "#### Clean and populate the staging load table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7IZLVaG_oZy"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "TRUNCATE TABLE `agentic_beans_raw_staging_load.telemetry_coffee_machine`;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO8IkzbI_wQK"
      },
      "source": [
        "Break the pipeline\n",
        "- Add \"° Celsius\" to some of the temperature values (boiler_temperature_celsius)\n",
        "- Add a \"%\" sign to some of the water reserve values (water_reservoir_level_percent)\n",
        "- Skew some of the water reserve values by dividing by 100.  So, instead of 34.82 we would have .3482 (water_reservoir_level_percent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pV3x8Yy_rYH"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "INSERT INTO `agentic_beans_raw_staging_load.telemetry_coffee_machine`\n",
        "(telemetry_coffee_machine_id, telemetry_load_id, machine_id, truck_id, telemetry_timestamp, boiler_temperature_celsius, brew_pressure_bar, water_flow_rate_ml_per_sec, grinder_motor_rpm, grinder_motor_torque_nm, water_reservoir_level_percent, bean_hopper_level_grams, total_brew_cycles_counter, last_error_code, last_error_description, power_consumption_watts, cleaning_cycle_status)\n",
        "SELECT CAST(GENERATE_UUID() AS STRING) AS telemetry_coffee_machine_id,\n",
        "       FORMAT_TIMESTAMP('%Y%m%d%H%M%S', CURRENT_TIMESTAMP()) || '_machine_batch' telemetry_load_id,\n",
        "       CAST(machine_id AS STRING) AS machine_id,\n",
        "       CAST(truck_id AS STRING) AS truck_id,\n",
        "       FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S %Z', CURRENT_TIMESTAMP()) AS telemetry_timestamp,\n",
        "\n",
        "       -- Break the coffee machine 'La Marzocco KB90 4-Group' as a firmware update\n",
        "       -- The machine will now say \"° Celsius\"\n",
        "       CASE WHEN truck_id IN (1,6,11,16) -- Coffee machine 'La Marzocco KB90 4-Group' as a firmware update\n",
        "            THEN CAST(boiler_temperature_celsius AS STRING) || '° Celsius'\n",
        "            ELSE CAST(boiler_temperature_celsius AS STRING)\n",
        "        END AS boiler_temperature_celsius,\n",
        "\n",
        "       CAST(brew_pressure_bar AS STRING) AS brew_pressure_bar,\n",
        "       CAST(water_flow_rate_ml_per_sec AS STRING) AS water_flow_rate_ml_per_sec,\n",
        "       CAST(grinder_motor_rpm AS STRING) AS grinder_motor_rpm,\n",
        "       CAST(grinder_motor_torque_nm AS STRING) AS grinder_motor_torque_nm,\n",
        "\n",
        "       -- Break the coffee machine 'La Marzocco KB90 4-Group' and 'Thermoplan Black&White3 CTS' as a firmware update\n",
        "       -- The La Marzocco KB90 4-Group will now send the percents as decimals (.4867 instead of 48.67)\n",
        "       -- The Thermoplan Black&White3 CTS will now send the percents with percent signs\n",
        "       CASE WHEN truck_id IN (1,6,11,16) -- Coffee machine 'La Marzocco KB90 4-Group' as a firmware update\n",
        "            THEN CAST(water_reservoir_level_percent / 100 AS STRING)\n",
        "            WHEN truck_id IN (5,10,15,20) -- Coffee machine 'Thermoplan Black&White3 CTS' as a firmware update\n",
        "            THEN CAST(water_reservoir_level_percent AS STRING) || '%'\n",
        "            ELSE CAST(water_reservoir_level_percent AS STRING)\n",
        "        END AS water_reservoir_level_percent,\n",
        "\n",
        "       CAST(bean_hopper_level_grams AS STRING) AS bean_hopper_level_grams,\n",
        "       CAST(total_brew_cycles_counter AS STRING) AS total_brew_cycles_counter,\n",
        "       CAST(last_error_code AS STRING) AS last_error_code,\n",
        "       CAST(last_error_description AS STRING) AS last_error_description,\n",
        "       CAST(power_consumption_watts AS STRING) AS power_consumption_watts,\n",
        "       CAST(cleaning_cycle_status AS STRING) AS cleaning_cycle_status\n",
        "\n",
        " FROM `agentic_beans_raw.telemetry_coffee_machine` AS telemetry_coffee_machine\n",
        "\n",
        "WHERE telemetry_timestamp = (SELECT max(telemetry_timestamp) FROM `agentic_beans_raw.telemetry_coffee_machine`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPbIHbQvYmH"
      },
      "source": [
        "#### Run the Data Quality Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S7abIaoAetz"
      },
      "outputs": [],
      "source": [
        "data_quality_scan_name = \"telemetry-coffee-machine-staging-load-dq\"\n",
        "data_quality_scan_job_name = startDataQualityScan(project_id, dataplex_region, data_quality_scan_name)\n",
        "print(f\"Data Quality Scan Started: {data_quality_scan_job_name}\")\n",
        "time.sleep(10)\n",
        "data_quality_scan_state = getStateDataQualityScan(project_id, dataplex_region, data_quality_scan_job_name)\n",
        "\n",
        "while data_quality_scan_state == \"PENDING\" or \\\n",
        "      data_quality_scan_state == \"STATE_UNSPECIFIED\" or \\\n",
        "      data_quality_scan_state == \"RUNNING\" or \\\n",
        "      data_quality_scan_state == \"CANCELING\":\n",
        "\n",
        "  print(f\"Data Quality Scan State: {data_quality_scan_state}\")\n",
        "  time.sleep(10)\n",
        "  data_quality_scan_state = getStateDataQualityScan(project_id, dataplex_region, data_quality_scan_job_name)\n",
        "\n",
        "if data_quality_scan_state == \"SUCCEEDED\":\n",
        "  print(f\"Data Quality Scan Completed Successfully\")\n",
        "else:\n",
        "  print(f\"Data Quality Scan Failed with status of: {data_quality_scan_state}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGCcbGcUBvb-"
      },
      "source": [
        "#### Check the Data Quality Job Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJNMnNE8Bvb_"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "SELECT data_quality_job_id\n",
        "  FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        " WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq'\n",
        "   AND job_start_time = (SELECT MAX(job_start_time)\n",
        "                           FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                          WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq')\n",
        "LIMIT 1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-qib7zCBvb_"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "SELECT COUNT(*) AS FailedCount\n",
        "FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "WHERE data_quality_job_id = (SELECT data_quality_job_id\n",
        "                               FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                              WHERE data_quality_scan. data_scan_id = 'telemetry-coffee-machine-staging-load-dq'\n",
        "                                AND job_start_time = (SELECT MAX(job_start_time)\n",
        "                                                        FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                                                       WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq')\n",
        "                              LIMIT 1)\n",
        "AND rule_passed = FALSE;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpiP0UShCL7h"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "-- Show the rows that failed\n",
        "SELECT data_source.dataset_id, data_source.table_id, rule_column, rule_parameters, rule_failed_records_query\n",
        "FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "WHERE data_quality_job_id = (SELECT data_quality_job_id\n",
        "                               FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                              WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq'\n",
        "                                AND job_start_time = (SELECT MAX(job_start_time)\n",
        "                                                        FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                                                       WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq')\n",
        "                              LIMIT 1)\n",
        "AND rule_passed = FALSE;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubay7jCoECK9"
      },
      "outputs": [],
      "source": [
        "%%bigquery failed_row_df\n",
        "\n",
        "-- Show the rows that failed\n",
        "SELECT data_source.dataset_id, data_source.table_id, rule_column, rule_parameters, rule_failed_records_query\n",
        "FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "WHERE data_quality_job_id = (SELECT data_quality_job_id\n",
        "                               FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                              WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq'\n",
        "                                AND job_start_time = (SELECT MAX(job_start_time)\n",
        "                                                        FROM `agentic_beans_raw_staging_load.telemetry_coffee_machine_data_quality`\n",
        "                                                       WHERE data_quality_scan.data_scan_id = 'telemetry-coffee-machine-staging-load-dq')\n",
        "                              LIMIT 1)\n",
        "AND rule_passed = FALSE;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj8E7y1PSrWu"
      },
      "source": [
        "#### Run the Dataform Pipeline: It should fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ex4baIvSrWv"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_original = compile_and_run_dataform_workflow(repository_name, workspace_name_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20A362jASrWv"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnfBGPDwSrWv"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_original_id=  workflow_invocation_original[\"results\"][\"name\"].split('/')[-1]\n",
        "time.sleep(30)  # should only take about 20 seconds to complete\n",
        "get_worflow_invocation_status(repository_name, workflow_invocation_original_id)[\"results\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWopfHp5B5fM"
      },
      "source": [
        "#### Generate the prompt to send to the Data Engineering Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsffbmB-KqGm"
      },
      "outputs": [],
      "source": [
        "def trim_trailing_zeros_string(s_number):\n",
        "    \"\"\"\n",
        "    Trims trailing zeros from a string representation of a number.\n",
        "    Assumes the input is a string.\n",
        "\n",
        "    BigQuery can return 10.030000000 for 10.03 and we do not need the trailing zeros which\n",
        "    will confuse the RegEx.\n",
        "    \"\"\"\n",
        "    if not isinstance(s_number, str):\n",
        "        s_number = str(s_number) # Convert to string if not already\n",
        "\n",
        "    s_number = s_number.rstrip('0')\n",
        "    if s_number.endswith('.'):\n",
        "        s_number = s_number.rstrip('.')\n",
        "    return s_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PqBNnCIB6mg"
      },
      "outputs": [],
      "source": [
        "data_engineering_agent_meta_prompt = \"\"\"I have the below column(s) that have failed a data quality check.\n",
        "For each column:\n",
        "- Generate a short and direct prompt for a data engineering agent to correct the ETL process.\n",
        "  - The prompt should be specific, starting with \"For {column-name}, please adjust the column by doing {fill me in}\".\n",
        "  - We cannot change the regular expression; we need to transform the data to meet the regular expression.\n",
        "  - If the data cannot be corrected, please let me know.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "column_prompts = \"\"\n",
        "\n",
        "for index, row in failed_row_df.iterrows():\n",
        "  dataset_id = row[\"dataset_id\"]\n",
        "  table_id = row[\"table_id\"]\n",
        "  rule_column = row[\"rule_column\"]\n",
        "  rule_parameters = row[\"rule_parameters\"]\n",
        "  rule_failed_records_query = row[\"rule_failed_records_query\"]\n",
        "\n",
        "  print(f\"dataset_id: {dataset_id}\")\n",
        "  print(f\"dataset_id: {table_id}\")\n",
        "  print(f\"dataset_id: {rule_column}\")\n",
        "  print(f\"dataset_id: {rule_parameters}\")\n",
        "  print(f\"dataset_id: {rule_failed_records_query}\")\n",
        "  print()\n",
        "\n",
        "  # Run the rule_failed_records_query (but replace SELECT * with field name)\n",
        "  sql = rule_failed_records_query.replace(\"SELECT *\",f\"SELECT {rule_column}\")\n",
        "  failed_row_data_df = RunQuery(sql)\n",
        "\n",
        "  bad_values_xml = f\"<invalid-values-for-{rule_column}>\\n\"\n",
        "  for index, row in failed_row_data_df.iterrows():\n",
        "    value = trim_trailing_zeros_string(row[f\"{rule_column}\"])\n",
        "    bad_values_xml += f\"<value>{value}</value>\\n\"\n",
        "  bad_values_xml += f\"</invalid-values-for-{rule_column}>\\n\"\n",
        "\n",
        "\n",
        "  # Run the rule_failed_records_query for SUCCESSFUL rows from the \"raw\" dataset which contains valid values\n",
        "  # Get the data from the most recent load.\n",
        "  sql = f\"\"\"SELECT {rule_column}\n",
        "             FROM `agentic_beans_raw.telemetry_coffee_machine`\n",
        "           WHERE telemetry_load_id = (SELECT telemetry_load_id\n",
        "                                         FROM `agentic_beans_raw.telemetry_coffee_machine`\n",
        "                                       WHERE telemetry_timestamp = (SELECT MAX(telemetry_timestamp)\n",
        "                                                                     FROM `agentic_beans_raw.telemetry_coffee_machine`)\n",
        "                                       LIMIT 1)\n",
        "           LIMIT 10;\"\"\"\n",
        "  failed_row_data_df = RunQuery(sql)\n",
        "\n",
        "  value_values_xml = f\"<valid-values-for-{rule_column}>\\n\"\n",
        "  for index, row in failed_row_data_df.iterrows():\n",
        "    value = trim_trailing_zeros_string(row[f\"{rule_column}\"])\n",
        "    value_values_xml += f\"<value>{value}</value>\\n\"\n",
        "  value_values_xml += f\"</valid-values-for-{rule_column}>\\n\"\n",
        "\n",
        "\n",
        "  column_prompts += f\"\"\"<column-{rule_column}>\n",
        "Column Name: {rule_column}\n",
        "RegEx: {rule_parameters}\n",
        "\n",
        "{value_values_xml}\n",
        "\n",
        "{bad_values_xml}\n",
        "\n",
        "</column-{rule_column}>\n",
        "\"\"\"\n",
        "\n",
        "data_engineering_agent_meta_prompt += column_prompts\n",
        "\n",
        "print(f\"data_engineering_agent_meta_prompt: {data_engineering_agent_meta_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq9IejXHFRz6"
      },
      "outputs": [],
      "source": [
        "#### Run Gemini\n",
        "response_schema = {\n",
        "  \"type\": \"object\",\n",
        "  \"required\": [\n",
        "    \"generated_prompts\"\n",
        "  ],\n",
        "  \"properties\": {\n",
        "    \"generated_prompts\": {\n",
        "      \"type\": \"array\",\n",
        "      \"items\": {\n",
        "        \"type\": \"object\",\n",
        "        \"required\": [\n",
        "          \"column_prompt\"\n",
        "        ],\n",
        "        \"properties\": {\n",
        "          \"column_prompt\": {\n",
        "            \"type\": \"string\"\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "generated_prompt_response = GeminiLLM(data_engineering_agent_meta_prompt, response_schema=response_schema)\n",
        "\n",
        "generated_prompt_dict = json.loads(generated_prompt_response)\n",
        "\n",
        "data_engineering_agent_prompt = \"\"\n",
        "for item in generated_prompt_dict[\"generated_prompts\"]:\n",
        "  data_engineering_agent_prompt += item[\"column_prompt\"] + \"\\n\"\n",
        "\n",
        "# Sample Output:\n",
        "# For boiler_temperature_celsius, please adjust the column by doing removing the '° Celsius' suffix from the values.\n",
        "# For water_reservoir_level_percent, please adjust the column by doing multiply the values by 100 if the value is between 0 and 1, and remove the '%' suffix from the values.\n",
        "print(f\"{data_engineering_agent_prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B2lYphJFTmf"
      },
      "source": [
        "#### Run the Data Engineering Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUtQii_yFVzZ"
      },
      "outputs": [],
      "source": [
        "data_engineering_agent_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65eAfv2EQwA0"
      },
      "outputs": [],
      "source": [
        "# NOTE: This requires ALLOWLISTING\n",
        "# Please DO NOT take a hard dependency on this API, it will be changing in the future and is just for testing.\n",
        "# Contact us for more information.\n",
        "\n",
        "url = f\"https://geminidataanalytics.googleapis.com/v1alpha1/projects/{project_id}/locations/global:run\"\n",
        "\n",
        "request_body = {\n",
        "  \"parent\": f\"projects/{project_id}/locations/global\",\n",
        "  \"pipeline_id\": f\"projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/workspaces/{workspace_name_auto}\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"user_message\": {\n",
        "        \"text\": data_engineering_agent_prompt\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "print(f\"request_body: {request_body}\")\n",
        "\n",
        "data_eng_agent_response = \"\"\n",
        "\n",
        "try:\n",
        "  data_eng_agent_response = rest_api_helper(url, \"POST\", request_body)\n",
        "except Exception as e:\n",
        "  data_eng_agent_response = f\"An error occurred while performing the data engineering task: {e}\"\n",
        "\n",
        "print(f\"data_eng_agent_response: {data_eng_agent_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esiOhQ9STXMS"
      },
      "outputs": [],
      "source": [
        "def llm_as_a_judge(original_prompt: str, response_from_processing: str) -> bool:\n",
        "    \"\"\"\n",
        "    Determines if the prompt has run correctly\n",
        "    \"\"\"\n",
        "    response_schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\"processing_status\": {\"type\": \"boolean\"}},\n",
        "        \"required\": [\"processing_status\"]\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"Respond back with True if you think the below process request executed successfully or False if it looks like it failed.\n",
        "\n",
        "    Processing Request: {original_prompt}\n",
        "\n",
        "    Response from Processing: {response_from_processing}\n",
        "    \"\"\"\n",
        "    gemini_response = GeminiLLM(prompt, response_schema=response_schema)\n",
        "    gemini_response_json = json.loads(gemini_response)\n",
        "    return bool(gemini_response_json[\"processing_status\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaVDkBFjTM0u"
      },
      "outputs": [],
      "source": [
        "# LLM as a Judge: Determine if our changes were successful\n",
        "llm_as_a_judge_result = llm_as_a_judge(data_engineering_agent_prompt, data_eng_agent_response)\n",
        "\n",
        "if llm_as_a_judge_result == True:\n",
        "  print(f\"The Data Engineering Agent successfully completed the pipeline correction.\")\n",
        "else:\n",
        "  print(f\"It appears the data engineering agent did not complete our desired results.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6iCSat0RT0I"
      },
      "outputs": [],
      "source": [
        "# Commit the changes to the Dataform repo\n",
        "\n",
        "# NOTE: Once you commit, you will need to \"rollback\" or copy the code from the \"Original\" code.\n",
        "#       You would copy the original code from \"telemetry-coffee-machine-original\": definitions/agentic_beans_raw/telemetry_coffee_machine.sqlx\n",
        "#       to the \"telemetry-coffee-machine-auto\" and commit the changes\n",
        "\n",
        "url = f\"https://dataform.googleapis.com/v1/projects/{project_id}/locations/{dataform_region}/repositories/{repository_name}/workspaces/{workspace_name_auto}:commit\"\n",
        "\n",
        "request_body = {\n",
        "    \"author\":{\n",
        "        \"name\":\"Admin User\",\n",
        "        \"emailAddress\":gcp_account_name\n",
        "        },\n",
        "    \"commitMessage\":\"Data Eng Agent Automation\"\n",
        "    }\n",
        "\n",
        "print(f\"request_body: {request_body}\")\n",
        "\n",
        "dataform_commit_response = rest_api_helper(url, \"POST\", request_body)\n",
        "\n",
        "print(f\"dataform_commit_response: {dataform_commit_response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nYYNdiHAj3e"
      },
      "source": [
        "#### Run the UPDATED Dataform Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjCuRmloAkIa"
      },
      "outputs": [],
      "source": [
        "# This will run the \"auto pipeline\" which was updated by the data engineering agent\n",
        "\n",
        "workflow_invocation_auto = compile_and_run_dataform_workflow(repository_name, workspace_name_auto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJ9K7bXNAthv"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_auto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lve3qPenAvl9"
      },
      "outputs": [],
      "source": [
        "workflow_invocation_auto_id=  workflow_invocation_auto[\"results\"][\"name\"].split('/')[-1]\n",
        "time.sleep(30)  # should only take about 20 seconds to complete\n",
        "get_worflow_invocation_status(repository_name, workflow_invocation_auto_id)[\"results\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4eL2q138uw8S",
        "b_hBijiyuw8V",
        "HMsUvoF4BP7Y",
        "m65vp54BUFRi",
        "JbOjdSP1kN9T",
        "9W8avEv6uw8X",
        "bI-KJELZ1jgt",
        "mWpDCqGFwO13",
        "pKsL3F3CvwTK",
        "K_kklVPwvW9D",
        "_TQjrzV1_o0N",
        "DzPbIHbQvYmH",
        "hj8E7y1PSrWu",
        "BWopfHp5B5fM",
        "42IxhtRRrvR-",
        "ASQ2BPisXDA0"
      ],
      "name": "Template",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
