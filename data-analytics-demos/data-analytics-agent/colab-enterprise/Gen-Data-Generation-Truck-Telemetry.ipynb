{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Cfv65rW2b7"
      },
      "source": [
        "### <font color='#4285f4'>Overview</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MpBBxscW2b7"
      },
      "source": [
        "Overview: Generates synthetic truck telemetry data\n",
        "\n",
        "Author:\n",
        "* Adam Paternostro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMsUvoF4BP7Y"
      },
      "source": [
        "### <font color='#4285f4'>License</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQgQkbOvj55d"
      },
      "source": [
        "```\n",
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m65vp54BUFRi"
      },
      "source": [
        "### <font color='#4285f4'>Pip installs</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MaWM6H5i6rX"
      },
      "outputs": [],
      "source": [
        "# PIP Installs (if necessary)\n",
        "import sys\n",
        "\n",
        "# !{sys.executable} -m pip install REPLACE-ME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmyL-Rg4Dr_f"
      },
      "source": [
        "### <font color='#4285f4'>Initialize</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOYsEVSXp6IP"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import HTML\n",
        "import IPython.display\n",
        "import google.auth\n",
        "import requests\n",
        "import json\n",
        "import uuid\n",
        "import base64\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import base64\n",
        "import random\n",
        "\n",
        "import logging\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, before_sleep_log, retry_if_exception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMlHl3bnkFPZ"
      },
      "outputs": [],
      "source": [
        "# Set these (run this cell to verify the output)\n",
        "\n",
        "bigquery_location = \"${bigquery_non_multi_region}\"\n",
        "region = \"${region}\"\n",
        "location = \"${location}\"\n",
        "\n",
        "\n",
        "# Get the current date and time\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "# Format the date and time as desired\n",
        "formatted_date = now.strftime(\"%Y-%m-%d-%H-%M\")\n",
        "\n",
        "# Get some values using gcloud\n",
        "project_id = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
        "user = !(gcloud auth list --filter=status:ACTIVE --format=\"value(account)\")\n",
        "\n",
        "if len(user) != 1:\n",
        "  raise RuntimeError(f\"user is not set: {user}\")\n",
        "user = user[0]\n",
        "\n",
        "print(f\"project_id = {project_id}\")\n",
        "print(f\"user = {user}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ6m_wGrK0YG"
      },
      "source": [
        "### <font color='#4285f4'>Helper Methods</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOjdSP1kN9T"
      },
      "source": [
        "#### restAPIHelper\n",
        "Calls the Google Cloud REST API using the current users credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40wlwnY4kM11"
      },
      "outputs": [],
      "source": [
        "def restAPIHelper(url: str, http_verb: str, request_body: str) -> str:\n",
        "  \"\"\"Calls the Google Cloud REST API passing in the current users credentials\"\"\"\n",
        "\n",
        "  import google.auth.transport.requests\n",
        "  import requests\n",
        "  import google.auth\n",
        "  import json\n",
        "\n",
        "  # Get an access token based upon the current user\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "    \"Content-Type\" : \"application/json\",\n",
        "    \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  if http_verb == \"GET\":\n",
        "    response = requests.get(url, headers=headers)\n",
        "  elif http_verb == \"POST\":\n",
        "    response = requests.post(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PUT\":\n",
        "    response = requests.put(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"PATCH\":\n",
        "    response = requests.patch(url, json=request_body, headers=headers)\n",
        "  elif http_verb == \"DELETE\":\n",
        "    response = requests.delete(url, headers=headers)\n",
        "  else:\n",
        "    raise RuntimeError(f\"Unknown HTTP verb: {http_verb}\")\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return json.loads(response.content)\n",
        "    #image_data = json.loads(response.content)[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "  else:\n",
        "    error = f\"Error restAPIHelper -> ' Status: '{response.status_code}' Text: '{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTYXm6hMW2b9"
      },
      "source": [
        "#### RetryCondition (for retrying LLM calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXWlRM-fW2b9"
      },
      "outputs": [],
      "source": [
        "def RetryCondition(error):\n",
        "  error_string = str(error)\n",
        "  print(error_string)\n",
        "\n",
        "  retry_errors = [\n",
        "      \"RESOURCE_EXHAUSTED\",\n",
        "      \"No content in candidate\",\n",
        "      # Add more error messages here as needed\n",
        "  ]\n",
        "\n",
        "  for retry_error in retry_errors:\n",
        "    if retry_error in error_string:\n",
        "      print(\"Retrying...\")\n",
        "      return True\n",
        "\n",
        "  return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOFTk6sj1YIV"
      },
      "source": [
        "#### Gemini LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHit3Hh-1ZAW"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_exponential(multiplier=1, min=1, max=60), stop=stop_after_attempt(10), retry=retry_if_exception(RetryCondition), before_sleep=before_sleep_log(logging.getLogger(), logging.INFO))\n",
        "def GeminiLLM(prompt, model = \"gemini-2.5-flash\", response_schema = None,\n",
        "                 temperature = 1, topP = 1, topK = 32):\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#supported_models\n",
        "\n",
        "  llm_response = None\n",
        "  if temperature < 0:\n",
        "    temperature = 0\n",
        "\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request() # required to acess access token\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "      \"Content-Type\" : \"application/json\",\n",
        "      \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference\n",
        "  url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model}:generateContent\"\n",
        "\n",
        "  generation_config = {\n",
        "    \"temperature\": temperature,\n",
        "    \"topP\": topP,\n",
        "    \"maxOutputTokens\": 8192,\n",
        "    \"candidateCount\": 1,\n",
        "    \"responseMimeType\": \"application/json\",\n",
        "  }\n",
        "\n",
        "  # Add inthe response schema for when it is provided\n",
        "  if response_schema is not None:\n",
        "    generation_config[\"responseSchema\"] = response_schema\n",
        "\n",
        "  if model == \"gemini-2.0-flash\":\n",
        "    generation_config[\"topK\"] = topK\n",
        "\n",
        "  payload = {\n",
        "    \"contents\": {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": {\n",
        "          \"text\": prompt\n",
        "      },\n",
        "    },\n",
        "    \"generation_config\": {\n",
        "      **generation_config\n",
        "    },\n",
        "    \"safety_settings\": {\n",
        "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    try:\n",
        "      json_response = json.loads(response.content)\n",
        "    except Exception as error:\n",
        "      raise RuntimeError(f\"An error occurred parsing the JSON: {error}\")\n",
        "\n",
        "    if \"candidates\" in json_response:\n",
        "      candidates = json_response[\"candidates\"]\n",
        "      if len(candidates) > 0:\n",
        "        candidate = candidates[0]\n",
        "        if \"content\" in candidate:\n",
        "          content = candidate[\"content\"]\n",
        "          if \"parts\" in content:\n",
        "            parts = content[\"parts\"]\n",
        "            if len(parts):\n",
        "              part = parts[0]\n",
        "              if \"text\" in part:\n",
        "                text = part[\"text\"]\n",
        "                llm_response = text\n",
        "              else:\n",
        "                raise RuntimeError(\"No text in part: {response.content}\")\n",
        "            else:\n",
        "              raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "          else:\n",
        "            raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "        else:\n",
        "          raise RuntimeError(\"No content in candidate: {response.content}\")\n",
        "      else:\n",
        "        raise RuntimeError(\"No candidates: {response.content}\")\n",
        "    else:\n",
        "      raise RuntimeError(\"No candidates: {response.content}\")\n",
        "\n",
        "    # Remove some typically response characters (if asking for a JSON reply)\n",
        "    llm_response = llm_response.replace(\"```json\",\"\")\n",
        "    llm_response = llm_response.replace(\"```\",\"\")\n",
        "    llm_response = llm_response.replace(\"\\n\",\"\")\n",
        "\n",
        "    return llm_response\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(f\"Error with prompt:'{prompt}'  Status:'{response.status_code}' Text:'{response.text}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWXSCd5VCPjf"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_exponential(multiplier=1, min=1, max=60), stop=stop_after_attempt(10), retry=retry_if_exception(RetryCondition), before_sleep=before_sleep_log(logging.getLogger(), logging.INFO))\n",
        "def GeminiLLM_VerifyImage(prompt, imageBase64, model = \"gemini-2.0-flash\", response_schema = None,\n",
        "                 temperature = 1, topP = 1, topK = 32):\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#supported_models\n",
        "\n",
        "  llm_response = None\n",
        "  if temperature < 0:\n",
        "    temperature = 0\n",
        "\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request() # required to acess access token\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "      \"Content-Type\" : \"application/json\",\n",
        "      \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference\n",
        "  url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model}:generateContent\"\n",
        "\n",
        "  generation_config = {\n",
        "    \"temperature\": temperature,\n",
        "    \"topP\": topP,\n",
        "    \"maxOutputTokens\": 8192,\n",
        "    \"candidateCount\": 1,\n",
        "    \"responseMimeType\": \"application/json\",\n",
        "  }\n",
        "\n",
        "  # Add inthe response schema for when it is provided\n",
        "  if response_schema is not None:\n",
        "    generation_config[\"responseSchema\"] = response_schema\n",
        "\n",
        "  if model == \"gemini-2.0-flash\":\n",
        "    generation_config[\"topK\"] = topK\n",
        "\n",
        "  payload = {\n",
        "    \"contents\": {\n",
        "      \"role\": \"user\",\n",
        "      \"parts\": [\n",
        "          { \"text\": prompt },\n",
        "          { \"inlineData\": {  \"mimeType\": \"image/png\", \"data\": f\"{imageBase64}\" } }\n",
        "        ]\n",
        "    },\n",
        "    \"generation_config\": {\n",
        "      **generation_config\n",
        "    },\n",
        "    \"safety_settings\": {\n",
        "      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\n",
        "    }\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    try:\n",
        "      json_response = json.loads(response.content)\n",
        "    except Exception as error:\n",
        "      raise RuntimeError(f\"An error occurred parsing the JSON: {error}\")\n",
        "\n",
        "    if \"candidates\" in json_response:\n",
        "      candidates = json_response[\"candidates\"]\n",
        "      if len(candidates) > 0:\n",
        "        candidate = candidates[0]\n",
        "        if \"content\" in candidate:\n",
        "          content = candidate[\"content\"]\n",
        "          if \"parts\" in content:\n",
        "            parts = content[\"parts\"]\n",
        "            if len(parts):\n",
        "              part = parts[0]\n",
        "              if \"text\" in part:\n",
        "                text = part[\"text\"]\n",
        "                llm_response = text\n",
        "              else:\n",
        "                raise RuntimeError(\"No text in part: {response.content}\")\n",
        "            else:\n",
        "              raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "          else:\n",
        "            raise RuntimeError(\"No parts in content: {response.content}\")\n",
        "        else:\n",
        "          raise RuntimeError(\"No content in candidate: {response.content}\")\n",
        "      else:\n",
        "        raise RuntimeError(\"No candidates: {response.content}\")\n",
        "    else:\n",
        "      raise RuntimeError(\"No candidates: {response.content}\")\n",
        "\n",
        "    # Remove some typically response characters (if asking for a JSON reply)\n",
        "    llm_response = llm_response.replace(\"```json\",\"\")\n",
        "    llm_response = llm_response.replace(\"```\",\"\")\n",
        "    llm_response = llm_response.replace(\"\\n\",\"\")\n",
        "\n",
        "    return llm_response\n",
        "\n",
        "  else:\n",
        "    raise RuntimeError(f\"Error with prompt:'{prompt}'  Status:'{response.status_code}' Text:'{response.text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0UGKGtYoiy4"
      },
      "source": [
        "#### Imagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vle77LwGomGF"
      },
      "outputs": [],
      "source": [
        "def ImageGen(prompt):\n",
        "  creds, project = google.auth.default()\n",
        "  auth_req = google.auth.transport.requests.Request()\n",
        "  creds.refresh(auth_req)\n",
        "  access_token=creds.token\n",
        "\n",
        "  headers = {\n",
        "      \"Content-Type\" : \"application/json\",\n",
        "      \"Authorization\" : \"Bearer \" + access_token\n",
        "  }\n",
        "\n",
        "  model_version = \"imagen-4.0-generate-preview-06-06\" # Preview Access Model\n",
        "\n",
        "  # https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/image-generation\n",
        "  # url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/google/models/imagegeneration:predict\"\n",
        "  url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/publishers/google/models/{model_version}:predict\"\n",
        "\n",
        "  payload = {\n",
        "    \"instances\": [\n",
        "      {\n",
        "        \"prompt\": prompt\n",
        "      }\n",
        "    ],\n",
        "    \"parameters\": {\n",
        "      \"sampleCount\": 1,\n",
        "      \"personGeneration\" : \"dont_allow\"  # change to allow_adult for people generation\n",
        "    }\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    response_json = json.loads(response.content)\n",
        "    # print(f\"Imagen3 response_json: {response_json}\")\n",
        "\n",
        "    if \"blocked\" in response_json:\n",
        "      print(f\"Blocked: {response_json['blocked']}\")\n",
        "\n",
        "    if \"predictions\" in response_json:\n",
        "      image_data = response_json[\"predictions\"][0][\"bytesBase64Encoded\"]\n",
        "      image_data = base64.b64decode(image_data)\n",
        "      filename= str(uuid.uuid4()) + \".png\"\n",
        "      with open(filename, \"wb\") as f:\n",
        "        f.write(image_data)\n",
        "      print(f\"Image generated OK.\")\n",
        "      return filename\n",
        "    else:\n",
        "      raise RuntimeError(f\"No predictions in response: {response.content}\")\n",
        "  else:\n",
        "    error = f\"Error with prompt:'{prompt}'  Status:'{response.status_code}' Text:'{response.text}'\"\n",
        "    raise RuntimeError(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8FaAkAFolpK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI-KJELZ1jgt"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmnCwYvA1kZv"
      },
      "outputs": [],
      "source": [
        "def RunQuery(sql):\n",
        "  import time\n",
        "  from google.cloud import bigquery\n",
        "  client = bigquery.Client()\n",
        "\n",
        "  if (sql.startswith(\"SELECT\") or sql.startswith(\"WITH\")):\n",
        "      df_result = client.query(sql).to_dataframe()\n",
        "      return df_result\n",
        "  else:\n",
        "    job_config = bigquery.QueryJobConfig(priority=bigquery.QueryPriority.INTERACTIVE)\n",
        "    query_job = client.query(sql, job_config=job_config)\n",
        "\n",
        "    # Check on the progress by getting the job's updated state.\n",
        "    query_job = client.get_job(\n",
        "        query_job.job_id, location=query_job.location\n",
        "    )\n",
        "    print(\"Job {} is currently in state {} with error result of {}\".format(query_job.job_id, query_job.state, query_job.error_result))\n",
        "\n",
        "    while query_job.state != \"DONE\":\n",
        "      time.sleep(2)\n",
        "      query_job = client.get_job(\n",
        "          query_job.job_id, location=query_job.location\n",
        "          )\n",
        "      print(\"Job {} is currently in state {} with error result of {}\".format(query_job.job_id, query_job.state, query_job.error_result))\n",
        "\n",
        "    if query_job.error_result == None:\n",
        "      return True\n",
        "    else:\n",
        "      raise Exception(query_job.error_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPz91mshqZBb"
      },
      "source": [
        "#### GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1T5-Rikqafm"
      },
      "outputs": [],
      "source": [
        "# This was generated by GenAI\n",
        "\n",
        "def copy_file_to_gcs(local_file_path, bucket_name, destination_blob_name):\n",
        "  \"\"\"Copies a file from a local drive to a GCS bucket.\n",
        "\n",
        "  Args:\n",
        "      local_file_path: The full path to the local file.\n",
        "      bucket_name: The name of the GCS bucket to upload to.\n",
        "      destination_blob_name: The desired name of the uploaded file in the bucket.\n",
        "\n",
        "  Returns:\n",
        "      None\n",
        "  \"\"\"\n",
        "\n",
        "  import os\n",
        "  from google.cloud import storage\n",
        "\n",
        "  # Ensure the file exists locally\n",
        "  if not os.path.exists(local_file_path):\n",
        "      raise FileNotFoundError(f\"Local file '{local_file_path}' not found.\")\n",
        "\n",
        "  # Create a storage client\n",
        "  storage_client = storage.Client()\n",
        "\n",
        "  # Get a reference to the bucket\n",
        "  bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "  # Create a blob object with the desired destination path\n",
        "  blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "  # Upload the file from the local filesystem\n",
        "  content_type = \"\"\n",
        "  if local_file_path.endswith(\".html\"):\n",
        "    content_type = \"text/html; charset=utf-8\"\n",
        "\n",
        "  if local_file_path.endswith(\".json\"):\n",
        "    content_type = \"application/json; charset=utf-8\"\n",
        "\n",
        "  if content_type == \"\":\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "  else:\n",
        "    blob.upload_from_filename(local_file_path, content_type = content_type)\n",
        "\n",
        "  print(f\"File '{local_file_path}' uploaded to GCS bucket '{bucket_name}' as '{destination_blob_name}.  Content-Type: {content_type}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c51M89g0Ejmz"
      },
      "source": [
        "### <font color='#4285f4'>MAIN CODE - Create IoT Tables</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urKhNc2NYzFy"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "CREATE SCHEMA IF NOT EXISTS `agentic_beans_raw` OPTIONS(location = 'us-central1');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4VCLUsRGxFk"
      },
      "source": [
        "#### Coffee Machine Telemetry Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThaNTLFHbEm7"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--drop table `agentic_beans_raw.telemetry_coffee_machine`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `agentic_beans_raw.telemetry_coffee_machine`\n",
        "(\n",
        "    telemetry_coffee_machine_id     STRING         NOT NULL OPTIONS(description=\"A unique identifier for each telemetry reading.\"),\n",
        "    telemetry_load_id               STRING         NOT NULL OPTIONS(description=\"A unique identifier for the batch load.\"),\n",
        "    machine_id                      INT64          NOT NULL OPTIONS(description=\"The foreign key referencing the 'machine_id' from the 'machine_dim' table.\"),\n",
        "    truck_id                        INT64          NOT NULL OPTIONS(description=\"The foreign key referencing the 'truck_id' from the 'truck' table, indicating which truck this machine belongs to.\"),\n",
        "    telemetry_timestamp             TIMESTAMP      NOT NULL OPTIONS(description=\"The timestamp when the telemetry reading was recorded by the machine.\"),\n",
        "    boiler_temperature_celsius      NUMERIC(5, 2)           OPTIONS(description=\"The current temperature of the machine's boiler in Celsius.\"),\n",
        "    brew_pressure_bar               NUMERIC(4, 2)           OPTIONS(description=\"The current pressure during brewing in bars.\"),\n",
        "    water_flow_rate_ml_per_sec      NUMERIC(5, 2)           OPTIONS(description=\"The water flow rate during brewing in milliliters per second.\"),\n",
        "    grinder_motor_rpm               INT64                   OPTIONS(description=\"The revolutions per minute (RPM) of the coffee grinder's motor.\"),\n",
        "    grinder_motor_torque_nm         NUMERIC(5, 2)           OPTIONS(description=\"The torque applied by the grinder motor in Newton-meters.\"),\n",
        "    water_reservoir_level_percent   NUMERIC(5, 2)           OPTIONS(description=\"The percentage of water remaining in the machine's reservoir.\"),\n",
        "    bean_hopper_level_grams         NUMERIC(8, 2)           OPTIONS(description=\"The quantity of coffee beans remaining in the hopper in grams.\"),\n",
        "    total_brew_cycles_counter       INT64                   OPTIONS(description=\"Cumulative count of brew cycles completed by the machine.\"),\n",
        "    last_error_code                 STRING                  OPTIONS(description=\"The most recent error code reported by the machine.\"),\n",
        "    last_error_description          STRING                  OPTIONS(description=\"A description for the most recent error code.\"),\n",
        "    power_consumption_watts         NUMERIC(8, 2)           OPTIONS(description=\"Current power consumption of the machine in Watts.\"),\n",
        "    cleaning_cycle_status           STRING                  OPTIONS(description=\"Current status of the cleaning cycle (e.g., 'completed', 'in_progress', 'due').\")\n",
        ")\n",
        "CLUSTER BY machine_id, telemetry_timestamp\n",
        "OPTIONS(\n",
        "    description=\"Raw telemetry data from coffee machines, used for real-time health monitoring, anomaly detection, and predictive maintenance insights.\"\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ63DzjTGsQz"
      },
      "source": [
        "#### Inventory Telemetry Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABzsQDoQMJ3Y"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--drop table `agentic_beans_raw.telemetry_inventory`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `agentic_beans_raw.telemetry_inventory`\n",
        "(\n",
        "    telemetry_inventory_id           STRING         NOT NULL OPTIONS(description=\"A unique identifier for each inventory update event.\"),\n",
        "    telemetry_load_id               STRING         NOT NULL OPTIONS(description=\"A unique identifier for the batch load.\"),\n",
        "    truck_id                        INT64          NOT NULL OPTIONS(description=\"The foreign key referencing the 'truck_id' from the 'truck' table.\"),\n",
        "    telemetry_timestamp             TIMESTAMP      NOT NULL OPTIONS(description=\"The timestamp when the inventory level was recorded or updated.\"),\n",
        "    ingredient_id                   INT64         NOT NULL OPTIONS(description=\"The foreign key referencing the 'ingredient_id' from the 'ingredient_dim' table.\"),\n",
        "    current_quantity_value          NUMERIC(10, 3) NOT NULL OPTIONS(description=\"The current measured quantity of the ingredient.\"),\n",
        "    unit_of_measure                 STRING         NOT NULL OPTIONS(description=\"The unit of measure for the quantity (e.g., 'grams', 'liters', 'count', 'sheets').\"),\n",
        "    event_type                      STRING         NOT NULL OPTIONS(description=\"The type of inventory event (e.g., 'sensor_reading', 'replenished', 'consumed_by_sale', 'waste', 'manual_adjustment').\"),\n",
        "    associated_transaction_id       STRING                  OPTIONS(description=\"Optional: ID of the POS transaction that caused a consumption event.\"),\n",
        "    source_sensor_id                STRING                  OPTIONS(description=\"Identifier for the specific sensor (e.g., weight sensor ID, RFID reader ID).\")\n",
        ")\n",
        "CLUSTER BY truck_id, telemetry_timestamp\n",
        "OPTIONS(\n",
        "    description=\"Raw inventory level updates for all consumables on coffee trucks, used for real-time stock management and replenishment planning.\"\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kg7adOzGo5h"
      },
      "source": [
        "#### Camera Telemetry Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xzZgYWhMQeg"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--drop table `agentic_beans_raw.telemetry_camera_vision`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `agentic_beans_raw.telemetry_camera_vision`\n",
        "(\n",
        "    telemetry_camera_vision_id      STRING         NOT NULL OPTIONS(description=\"A unique identifier for each queue analysis event.\"),\n",
        "    telemetry_load_id               STRING         NOT NULL OPTIONS(description=\"A unique identifier for the batch load.\"),\n",
        "    truck_id                        INT64          NOT NULL OPTIONS(description=\"The foreign key referencing the 'truck_id' from the 'truck' table.\"),\n",
        "    telemetry_timestamp             TIMESTAMP      NOT NULL OPTIONS(description=\"The timestamp when the camera analysis was performed.\"),\n",
        "    camera_id                       INT64         NOT NULL OPTIONS(description=\"The foreign key referencing the 'camera_id' from the 'camera_dim' table.\"),\n",
        "    people_in_queue_count           INT64          NOT NULL OPTIONS(description=\"The number of people detected in the coffee truck's service queue.\"),\n",
        "    foot_traffic_count_nearby       INT64          NOT NULL OPTIONS(description=\"The number of people detected walking by the truck in the immediate vicinity.\"),\n",
        "    ai_detection_confidence_score   NUMERIC(3, 2)           OPTIONS(description=\"The AI model's confidence score for the detection accuracy (0.0 to 1.0).\"),\n",
        "    image_reference_url             STRING                  OPTIONS(description=\"Optional URL to the raw image or a key to the image in storage for audit/debugging.\"),\n",
        "    detection_model_version         STRING                  OPTIONS(description=\"The version of the AI model used for detection.\")\n",
        ")\n",
        "CLUSTER BY truck_id, telemetry_timestamp\n",
        "OPTIONS(\n",
        "    description=\"Raw data from AI-powered camera analysis, tracking customer queue lengths and general foot traffic around coffee trucks.\"\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwsCu0MqGj5C"
      },
      "source": [
        "#### Machine Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw_noMYVMVEK"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--drop table `agentic_beans_raw.machine_dim`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `agentic_beans_raw.machine`\n",
        "(\n",
        "    machine_id              INT64         NOT NULL OPTIONS(description=\"The unique identifier for the coffee machine.\"),\n",
        "    machine_model           STRING                  OPTIONS(description=\"The model name of the coffee machine (e.g., 'EspressoBot 3000', 'BrewMaster 500').\"),\n",
        "    manufacturer            STRING                  OPTIONS(description=\"The manufacturer of the coffee machine.\"),\n",
        "    serial_number           STRING                  OPTIONS(description=\"The manufacturer's serial number for the machine.\"),\n",
        "    installation_date       DATE                    OPTIONS(description=\"The date the machine was installed in a truck.\"),\n",
        "    last_maintenance_date   DATE                    OPTIONS(description=\"The date of the last recorded maintenance activity on the machine.\"),\n",
        "    status                  STRING                  OPTIONS(description=\"Current operational status of the machine (e.g., 'active', 'in_maintenance', 'retired').\")\n",
        ")\n",
        "CLUSTER BY machine_id\n",
        "OPTIONS(\n",
        "    description=\"Dimension table containing static information about each coffee machine in the fleet.\"\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS_ooJooSKFm"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "INSERT INTO `agentic_beans_raw.machine`\n",
        "(machine_id, machine_model, manufacturer, serial_number, installation_date, last_maintenance_date, status)\n",
        "VALUES\n",
        "(1, 'La Marzocco KB90 4-Group', 'La Marzocco', 'LMKB90-001', '2020-01-01', '2025-07-01', 'active'),\n",
        "(2, 'Slayer Espresso Steam LP 3-Group', 'Slayer Espresso', 'SESL3-002', '2020-01-01', '2025-06-25', 'active'),\n",
        "(3, 'Franke A1000 FM CM', 'Franke Coffee Systems', 'FA1KC-003', '2020-01-01', '2025-07-10', 'active'),\n",
        "(4, 'Nuova Simonelli Mythos II', 'Nuova Simonelli', 'NSM2-004', '2020-01-01', '2025-05-30', 'active'),\n",
        "(5, 'Thermoplan Black&White3 CTS', 'Thermoplan AG', 'TBW3C-005', '2020-01-01', '2025-07-15', 'active'),\n",
        "(6, 'La Marzocco KB90 4-Group', 'La Marzocco', 'LMKB90-006', '2020-01-01', '2025-06-10', 'active'),\n",
        "(7, 'Slayer Espresso Steam LP 3-Group', 'Slayer Espresso', 'SESL3-007', '2020-01-01', '2025-07-20', 'active'),\n",
        "(8, 'Franke A1000 FM CM', 'Franke Coffee Systems', 'FA1KC-008', '2020-01-01', '2025-06-05', 'active'),\n",
        "(9, 'Nuova Simonelli Mythos II', 'Nuova Simonelli', 'NSM2-009', '2020-01-01', '2025-07-02', 'active'),\n",
        "(10, 'Thermoplan Black&White3 CTS', 'Thermoplan AG', 'TBW3C-010', '2020-01-01', '2025-06-28', 'active'),\n",
        "(11, 'La Marzocco KB90 4-Group', 'La Marzocco', 'LMKB90-011', '2020-01-01', '2025-07-08', 'active'),\n",
        "(12, 'Slayer Espresso Steam LP 3-Group', 'Slayer Espresso', 'SESL3-012', '2020-01-01', '2025-05-20', 'active'),\n",
        "(13, 'Franke A1000 FM CM', 'Franke Coffee Systems', 'FA1KC-013', '2020-01-01', '2025-07-05', 'active'),\n",
        "(14, 'Nuova Simonelli Mythos II', 'Nuova Simonelli', 'NSM2-014', '2020-01-01', '2025-06-18', 'active'),\n",
        "(15, 'Thermoplan Black&White3 CTS', 'Thermoplan AG', 'TBW3C-015', '2020-01-01', '2025-07-22', 'active'),\n",
        "(16, 'La Marzocco KB90 4-Group', 'La Marzocco', 'LMKB90-016', '2020-01-01', '2025-06-14', 'active'),\n",
        "(17, 'Slayer Espresso Steam LP 3-Group', 'Slayer Espresso', 'SESL3-017', '2020-01-01', '2025-07-09', 'active'),\n",
        "(18, 'Franke A1000 FM CM', 'Franke Coffee Systems', 'FA1KC-018', '2020-01-01', '2025-05-25', 'active'),\n",
        "(19, 'Nuova Simonelli Mythos II', 'Nuova Simonelli', 'NSM2-019', '2020-01-01', '2025-07-11', 'active'),\n",
        "(20, 'Thermoplan Black&White3 CTS', 'Thermoplan AG', 'TBW3C-020', '2020-01-01', '2025-06-01', 'active');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6gaiWQ6Gd3f"
      },
      "source": [
        "#### Ingredient Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INb9BNP7MY8Y"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--drop table `agentic_beans_raw.ingredient_dim`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `agentic_beans_raw.ingredient`\n",
        "(\n",
        "    ingredient_id           INT64         NOT NULL OPTIONS(description=\"A unique identifier for each distinct ingredient.\"),\n",
        "    ingredient_name         STRING         NOT NULL OPTIONS(description=\"The common name of the ingredient (e.g., 'Espresso Beans', 'Dairy Milk Whole', 'Chocolate Syrup').\"),\n",
        "    ingredient_category     STRING                  OPTIONS(description=\"The category of the ingredient (e.g., 'Coffee Beans', 'Milk', 'Syrup', 'Packaging').\"),\n",
        "    standard_unit_of_measure STRING                  OPTIONS(description=\"The primary unit of measure for this ingredient (e.g., 'grams', 'liters', 'count').\"),\n",
        "    is_perishable           BOOL                    OPTIONS(description=\"Indicates if the ingredient is perishable (TRUE/FALSE).\")\n",
        ")\n",
        "CLUSTER BY ingredient_id\n",
        "OPTIONS(\n",
        "    description=\"Dimension table listing all trackable ingredients and consumables used on coffee trucks.\"\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r00EAckbRjQ9"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "INSERT INTO `agentic_beans_raw.ingredient`\n",
        "(ingredient_id, ingredient_name, ingredient_category, standard_unit_of_measure, is_perishable)\n",
        "VALUES\n",
        "-- Coffee Beans (8 types)\n",
        "(1, 'Signature Espresso Blend', 'Coffee Beans', 'grams', TRUE),\n",
        "(2, 'Single Origin Colombian Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "(3, 'Decaf House Blend Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "(4, 'Dark Roast French Press Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "(5, 'Light Roast Pour Over Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "(6, 'Nitro Cold Brew Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "(7, 'Seasonal Limited Edition Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "(8, 'Breakfast Blend Beans', 'Coffee Beans', 'grams', TRUE),\n",
        "\n",
        "-- Coffee Concentrates/Base (2 types)\n",
        "(9, 'Cold Brew Concentrate (Regular)', 'Coffee Liquid', 'liters', TRUE),\n",
        "(10, 'Nitro Cold Brew Concentrate', 'Coffee Liquid', 'liters', TRUE),\n",
        "\n",
        "-- Milk & Dairy Alternatives (6 types)\n",
        "(11, 'Whole Dairy Milk', 'Milk', 'liters', TRUE),\n",
        "(12, 'Skim Dairy Milk', 'Milk', 'liters', TRUE),\n",
        "(13, 'Organic Oat Milk', 'Milk', 'liters', TRUE),\n",
        "(14, 'Barista Blend Almond Milk', 'Milk', 'liters', TRUE),\n",
        "(15, 'Coconut Milk', 'Milk', 'liters', TRUE),\n",
        "(16, 'Soy Milk Unsweetened', 'Milk', 'liters', TRUE),\n",
        "\n",
        "-- Syrups & Sauces (12 types)\n",
        "(17, 'Classic Vanilla Syrup', 'Syrup', 'milliliters', TRUE),\n",
        "(18, 'Sugar-Free Vanilla Syrup', 'Syrup', 'milliliters', TRUE),\n",
        "(19, 'Caramel Sauce', 'Sauce', 'milliliters', TRUE),\n",
        "(20, 'Chocolate Mocha Sauce', 'Sauce', 'milliliters', TRUE),\n",
        "(21, 'White Chocolate Sauce', 'Sauce', 'milliliters', TRUE),\n",
        "(22, 'Hazelnut Syrup', 'Syrup', 'milliliters', TRUE),\n",
        "(23, 'Toasted Marshmallow Syrup', 'Syrup', 'milliliters', TRUE),\n",
        "(24, 'Spiced Chai Concentrate', 'Syrup', 'liters', TRUE),\n",
        "(25, 'Pumpkin Spice Sauce', 'Sauce', 'milliliters', TRUE), -- Seasonal\n",
        "(26, 'Peppermint Bark Syrup', 'Syrup', 'milliliters', TRUE), -- Seasonal\n",
        "(27, 'Agave Nectar', 'Sweetener Liquid', 'milliliters', TRUE),\n",
        "(28, 'Brown Sugar Cinnamon Syrup', 'Syrup', 'milliliters', TRUE),\n",
        "\n",
        "-- Other Beverages / Bases (2 types)\n",
        "(29, 'Hot Cocoa Mix', 'Beverage Powder', 'grams', FALSE),\n",
        "(30, 'Matcha Green Tea Powder', 'Tea', 'grams', FALSE),\n",
        "\n",
        "-- Consumables / Toppings (10 types)\n",
        "(31, 'Standard 12oz Hot Cup', 'Packaging', 'count', FALSE),\n",
        "(32, 'Standard 16oz Hot Cup', 'Packaging', 'count', FALSE),\n",
        "(33, 'Standard 20oz Cold Cup', 'Packaging', 'count', FALSE),\n",
        "(34, 'Hot Cup Lids (Mixed Size)', 'Packaging', 'count', FALSE),\n",
        "(35, 'Cold Cup Lids (Domed)', 'Packaging', 'count', FALSE),\n",
        "(36, 'Coffee Sleeves', 'Packaging', 'count', FALSE),\n",
        "(37, 'Stir Sticks (Wood)', 'Packaging', 'count', FALSE),\n",
        "(38, 'Sugar Packets', 'Sweetener', 'count', FALSE),\n",
        "(39, 'Splenda Sweetener Packets', 'Sweetener', 'count', FALSE),\n",
        "(40, 'Napkins (Recycled)', 'Packaging', 'count', FALSE);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-fA4k70GZ5w"
      },
      "source": [
        "#### Camera Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dJh1wTxMdS-"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "--drop table `agentic_beans_raw.camera_dim`;\n",
        "\n",
        "CREATE TABLE IF NOT EXISTS `agentic_beans_raw.camera`\n",
        "(\n",
        "    camera_id               INT64         NOT NULL OPTIONS(description=\"The unique identifier for the camera.\"),\n",
        "    camera_model            STRING                  OPTIONS(description=\"The model name of the camera.\"),\n",
        "    camera_location_type    STRING                  OPTIONS(description=\"The mounting location/type of the camera (e.g., 'front_facing_queue', 'side_foot_traffic').\"),\n",
        "    resolution_pixels       STRING                  OPTIONS(description=\"The resolution of the camera in pixels (e.g., '1920x1080').\"),\n",
        "    field_of_view_degrees   NUMERIC(5, 2)           OPTIONS(description=\"The camera's field of view in degrees.\"),\n",
        "    installation_date       DATE                    OPTIONS(description=\"The date the camera was installed.\"),\n",
        "    status                  STRING                  OPTIONS(description=\"Current operational status of the camera (e.g., 'online', 'offline', 'calibrating').\")\n",
        ")\n",
        "CLUSTER BY camera_id\n",
        "OPTIONS(\n",
        "    description=\"Dimension table containing static information about cameras used for customer queue and foot traffic analysis.\"\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKe_3ExmQ5st"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "INSERT INTO `agentic_beans_raw.camera`\n",
        "(camera_id, camera_model, camera_location_type, resolution_pixels, field_of_view_degrees, installation_date, status)\n",
        "VALUES\n",
        "-- Truck 1\n",
        "(1, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-01-05', 'online'),\n",
        "(2, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-01-05', 'online'),\n",
        "-- Truck 2\n",
        "(3, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-01-10', 'online'),\n",
        "(4, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-01-10', 'online'),\n",
        "-- Truck 3\n",
        "(5, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-01-15', 'online'),\n",
        "(6, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-01-15', 'online'),\n",
        "-- Truck 4\n",
        "(7, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-01-20', 'online'),\n",
        "(8, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-01-20', 'online'),\n",
        "-- Truck 5\n",
        "(9, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-02-01', 'online'),\n",
        "(10, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-02-01', 'online'),\n",
        "-- Truck 6\n",
        "(11, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-02-05', 'online'),\n",
        "(12, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-02-05', 'online'),\n",
        "-- Truck 7\n",
        "(13, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-02-10', 'online'),\n",
        "(14, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-02-10', 'online'),\n",
        "-- Truck 8\n",
        "(15, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-02-15', 'online'),\n",
        "(16, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-02-15', 'online'),\n",
        "-- Truck 9\n",
        "(17, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-03-01', 'online'),\n",
        "(18, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-03-01', 'online'),\n",
        "-- Truck 10\n",
        "(19, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-03-05', 'online'),\n",
        "(20, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-03-05', 'online'),\n",
        "-- Truck 11\n",
        "(21, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-03-10', 'online'),\n",
        "(22, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-03-10', 'online'),\n",
        "-- Truck 12\n",
        "(23, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-03-15', 'online'),\n",
        "(24, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-03-15', 'online'),\n",
        "-- Truck 13\n",
        "(25, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-04-01', 'online'),\n",
        "(26, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-04-01', 'online'),\n",
        "-- Truck 14\n",
        "(27, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-04-05', 'online'),\n",
        "(28, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-04-05', 'online'),\n",
        "-- Truck 15\n",
        "(29, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-04-10', 'online'),\n",
        "(30, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-04-10', 'online'),\n",
        "-- Truck 16\n",
        "(31, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-04-15', 'online'),\n",
        "(32, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-04-15', 'online'),\n",
        "-- Truck 17\n",
        "(33, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-05-01', 'online'),\n",
        "(34, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-05-01', 'online'),\n",
        "-- Truck 18\n",
        "(35, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-05-05', 'online'),\n",
        "(36, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-05-05', 'online'),\n",
        "-- Truck 19\n",
        "(37, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-05-10', 'online'),\n",
        "(38, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-05-10', 'online'),\n",
        "-- Truck 20\n",
        "(39, 'EyeOnDemand-XT', 'exterior_queue', '1920x1080', 120.0, '2023-05-15', 'online'),\n",
        "(40, 'CabinView 300', 'interior_cabin', '1280x720', 90.0, '2023-05-15', 'online');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_9k5mm7GU_H"
      },
      "source": [
        "#### Generate Coffee Machine Telemetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qP6zFKR1Vmi"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_FILENAME_MACHINE = 'telemetry_coffee_machine_raw.csv'\n",
        "START_DATE = datetime(2020, 1, 1, 0, 0, 0)\n",
        "END_DATE = datetime(2027, 1, 1, 0, 0, 0)\n",
        "INTERVAL_SECONDS_READING = 15 # Data reading interval is now every 15 seconds\n",
        "INTERVAL_MINUTES_BATCH = 5   # Batch upload ID is still generated every 5 minutes\n",
        "NUM_TRUCKS = 20              # Corresponds to machine_id 1-20 and truck_id 1-20\n",
        "\n",
        "# --- Normal Operating Ranges (for clean data) ---\n",
        "NORMAL_RANGES = {\n",
        "    'boiler_temperature_celsius': {'min': 92.0, 'max': 96.0},\n",
        "    'brew_pressure_bar': {'min': 8.8, 'max': 9.2},\n",
        "    'water_flow_rate_ml_per_sec': {'min': 1.2, 'max': 1.8},\n",
        "    'grinder_motor_rpm': {'min': 1450, 'max': 1750},\n",
        "    'grinder_motor_torque_nm': {'min': 0.9, 'max': 1.4},\n",
        "    'power_consumption_watts': {'min': 2800, 'max': 4200},\n",
        "}\n",
        "\n",
        "# --- Initial State & Simulation Parameters for each machine ---\n",
        "machine_states = {}\n",
        "for i in range(1, NUM_TRUCKS + 1):\n",
        "    machine_states[i] = {\n",
        "        'water_level_percent': 100.0,\n",
        "        'bean_level_grams': 2000.0,\n",
        "        'total_brew_cycles': 0\n",
        "    }\n",
        "\n",
        "# Parameters for simulating consumption and refills - adjusted for 15-second interval\n",
        "# These rates are per 15 seconds, so they are much smaller than previous per-minute rates\n",
        "WATER_CONSUMPTION_PER_15S = {'min': 0.0025, 'max': 0.0125} # % of reservoir per 15 seconds (e.g., 0.01% - 0.05% of 100%)\n",
        "BEAN_CONSUMPTION_PER_15S = {'min': 0.25, 'max': 1.25}      # grams per 15 seconds (e.g., 1-5g per minute)\n",
        "BREW_CYCLES_PER_15S = {'min': 0, 'max': 0.25}             # new cycles per 15 seconds (averaging 0-1 brew per minute)\n",
        "WATER_REFILL_THRESHOLD = 10.0 # % below which a refill happens\n",
        "BEAN_REFILL_THRESHOLD = 200.0 # grams below which a refill happens\n",
        "WATER_REFILL_LEVEL = 100.0\n",
        "BEAN_REFILL_LEVEL = 2000.0\n",
        "\n",
        "# --- CSV Header ---\n",
        "CSV_HEADER_MACHINE = [\n",
        "    \"telemetry_coffee_machine_id\",\n",
        "    \"telemetry_load_id\",\n",
        "    \"machine_id\",\n",
        "    \"truck_id\",\n",
        "    \"telemetry_timestamp\",\n",
        "    \"boiler_temperature_celsius\",\n",
        "    \"brew_pressure_bar\",\n",
        "    \"water_flow_rate_ml_per_sec\",\n",
        "    \"grinder_motor_rpm\",\n",
        "    \"grinder_motor_torque_nm\",\n",
        "    \"water_reservoir_level_percent\",\n",
        "    \"bean_hopper_level_grams\",\n",
        "    \"total_brew_cycles_counter\",\n",
        "    \"last_error_code\",\n",
        "    \"last_error_description\",\n",
        "    \"power_consumption_watts\",\n",
        "    \"cleaning_cycle_status\"\n",
        "]\n",
        "\n",
        "# --- Main Data Generation Logic for Coffee Machine Telemetry ---\n",
        "def generate_telemetry_coffee_machine_data():\n",
        "    \"\"\"\n",
        "    Generates telemetry data for coffee machines (15-second granularity, 5-min batch ID).\n",
        "    \"\"\"\n",
        "    print(f\"Starting coffee machine telemetry generation for {NUM_TRUCKS} trucks from {START_DATE} to {END_DATE}...\")\n",
        "    print(f\"Output file: {OUTPUT_FILENAME_MACHINE}\")\n",
        "    print(f\"Generating data every {INTERVAL_SECONDS_READING} second(s), with batch IDs every {INTERVAL_MINUTES_BATCH} minutes.\")\n",
        "\n",
        "    with open(OUTPUT_FILENAME_MACHINE, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(CSV_HEADER_MACHINE) # Write the header row\n",
        "\n",
        "        current_timestamp = START_DATE\n",
        "        row_count = 0\n",
        "        current_batch_load_id = None\n",
        "\n",
        "        while current_timestamp < END_DATE:\n",
        "            # Generate a new batch ID every INTERVAL_MINUTES_BATCH minutes\n",
        "            # This ensures the batch ID changes at 0, 5, 10, 15... minutes of each hour\n",
        "            if current_timestamp.minute % INTERVAL_MINUTES_BATCH == 0 and current_timestamp.second == 0:\n",
        "                 current_batch_load_id = current_timestamp.strftime('%Y%m%d%H%M%S') + '_machine_batch'\n",
        "\n",
        "            for truck_id in range(1, NUM_TRUCKS + 1):\n",
        "                machine_id = truck_id\n",
        "\n",
        "                state = machine_states[machine_id]\n",
        "\n",
        "                # Simulate water consumption and refills\n",
        "                state['water_level_percent'] -= random.uniform(WATER_CONSUMPTION_PER_15S['min'], WATER_CONSUMPTION_PER_15S['max'])\n",
        "                if state['water_level_percent'] <= WATER_REFILL_THRESHOLD:\n",
        "                    state['water_level_percent'] = WATER_REFILL_LEVEL - random.uniform(0.1, 0.5)\n",
        "\n",
        "                # Simulate bean consumption and refills\n",
        "                state['bean_level_grams'] -= random.uniform(BEAN_CONSUMPTION_PER_15S['min'], BEAN_CONSUMPTION_PER_15S['max'])\n",
        "                if state['bean_level_grams'] <= BEAN_REFILL_THRESHOLD:\n",
        "                    state['bean_level_grams'] = BEAN_REFILL_LEVEL - random.uniform(5, 10)\n",
        "\n",
        "                # Simulate cumulative brew cycles (round to nearest integer for realistic count)\n",
        "                state['total_brew_cycles'] += round(random.uniform(BREW_CYCLES_PER_15S['min'], BREW_CYCLES_PER_15S['max']))\n",
        "\n",
        "                # Generate telemetry values within normal ranges\n",
        "                boiler_temp = round(random.uniform(NORMAL_RANGES['boiler_temperature_celsius']['min'], NORMAL_RANGES['boiler_temperature_celsius']['max']), 2)\n",
        "                brew_pressure = round(random.uniform(NORMAL_RANGES['brew_pressure_bar']['min'], NORMAL_RANGES['brew_pressure_bar']['max']), 2)\n",
        "                water_flow = round(random.uniform(NORMAL_RANGES['water_flow_rate_ml_per_sec']['min'], NORMAL_RANGES['water_flow_rate_ml_per_sec']['max']), 2)\n",
        "                grinder_rpm = random.randint(NORMAL_RANGES['grinder_motor_rpm']['min'], NORMAL_RANGES['grinder_motor_rpm']['max'])\n",
        "                grinder_torque = round(random.uniform(NORMAL_RANGES['grinder_motor_torque_nm']['min'], NORMAL_RANGES['grinder_motor_torque_nm']['max']), 2)\n",
        "                power_watts = random.randint(NORMAL_RANGES['power_consumption_watts']['min'], NORMAL_RANGES['power_consumption_watts']['max'])\n",
        "\n",
        "                last_error_code = None\n",
        "                last_error_description = None\n",
        "                cleaning_cycle_status = 'not_due'\n",
        "\n",
        "                row = [\n",
        "                    str(uuid.uuid4()),\n",
        "                    current_batch_load_id,\n",
        "                    machine_id,\n",
        "                    truck_id,\n",
        "                    current_timestamp.isoformat(),\n",
        "                    boiler_temp,\n",
        "                    brew_pressure,\n",
        "                    water_flow,\n",
        "                    grinder_rpm,\n",
        "                    grinder_torque,\n",
        "                    round(state['water_level_percent'], 2),\n",
        "                    round(state['bean_level_grams'], 2),\n",
        "                    state['total_brew_cycles'],\n",
        "                    last_error_code,\n",
        "                    last_error_description,\n",
        "                    power_watts,\n",
        "                    cleaning_cycle_status\n",
        "                ]\n",
        "                writer.writerow(row)\n",
        "                row_count += 1\n",
        "\n",
        "            current_timestamp += timedelta(seconds=INTERVAL_SECONDS_READING)\n",
        "\n",
        "            # Print progress update (e.g., every 100,000 rows)\n",
        "            if row_count % 100000 == 0:\n",
        "                print(f\"Generated {row_count} rows up to {current_timestamp}...\")\n",
        "\n",
        "    print(f\"\\nCoffee machine telemetry generation complete! Total rows generated: {row_count}. File saved to: {OUTPUT_FILENAME_MACHINE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcmxuvEe1nLv"
      },
      "outputs": [],
      "source": [
        "generate_telemetry_coffee_machine_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiFaegUl1qE4"
      },
      "outputs": [],
      "source": [
        "!head telemetry_coffee_machine_raw.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHEf8Dix1x4O"
      },
      "outputs": [],
      "source": [
        "!gsutil cp telemetry_coffee_machine_raw.csv gs://gcs-bucket-namet/telemetry_coffee_machine_raw/telemetry_coffee_machine_raw.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIVMjBkIfEj4"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "LOAD DATA INTO `agentic_beans_raw.telemetry_coffee_machine`\n",
        "(\n",
        "    telemetry_coffee_machine_id     STRING,\n",
        "    telemetry_load_id               STRING,\n",
        "    machine_id                      INT64,\n",
        "    truck_id                        INT64,\n",
        "    telemetry_timestamp             TIMESTAMP,\n",
        "    boiler_temperature_celsius      NUMERIC(5, 2),\n",
        "    brew_pressure_bar               NUMERIC(4, 2),\n",
        "    water_flow_rate_ml_per_sec      NUMERIC(5, 2),\n",
        "    grinder_motor_rpm               INT64,\n",
        "    grinder_motor_torque_nm         NUMERIC(5, 2),\n",
        "    water_reservoir_level_percent   NUMERIC(5, 2),\n",
        "    bean_hopper_level_grams         NUMERIC(8, 2),\n",
        "    total_brew_cycles_counter       INT64,\n",
        "    last_error_code                 STRING,\n",
        "    last_error_description          STRING,\n",
        "    power_consumption_watts         NUMERIC(8, 2),\n",
        "    cleaning_cycle_status           STRING\n",
        "\n",
        ")\n",
        "FROM FILES (\n",
        " format = 'csv',\n",
        " skip_leading_rows = 1,\n",
        " uris = ['gs://gcs-bucket-namet/telemetry_coffee_machine_raw/telemetry_coffee_machine_raw.csv']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppqOlZkiGNr6"
      },
      "source": [
        "#### Generate Inventory Telemetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlTUIqj88tG2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_FILENAME_INVENTORY = 'telemetry_inventory_raw.csv'\n",
        "START_DATE = datetime(2020, 1, 1, 0, 0, 0)\n",
        "END_DATE = datetime(2027, 1, 1, 0, 0, 0)\n",
        "INTERVAL_MINUTES_READING = 1 # Data reading interval is now every 1 minute\n",
        "INTERVAL_MINUTES_BATCH = 5   # Batch upload ID is still generated every 5 minutes\n",
        "NUM_TRUCKS = 20\n",
        "\n",
        "# --- Ingredient Definitions and Simulation Parameters ---\n",
        "INGREDIENT_PROPERTIES = {\n",
        "    # Coffee Beans (grams) - consumption per minute\n",
        "    1: {'name': 'Signature Espresso Blend', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (2000, 5000), 'consumption': (2, 10), 'refill_threshold': (200, 500), 'refill_qty': (2000, 5000), 'sensor_type': 'weight_sensor_bean'},\n",
        "    2: {'name': 'Single Origin Colombian Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (1000, 3000), 'consumption': (1, 6), 'refill_threshold': (100, 300), 'refill_qty': (1000, 3000), 'sensor_type': 'weight_sensor_bean'},\n",
        "    3: {'name': 'Decaf House Blend Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (500, 1500), 'consumption': (0.5, 3), 'refill_threshold': (50, 150), 'refill_qty': (500, 1500), 'sensor_type': 'weight_sensor_bean'},\n",
        "    4: {'name': 'Dark Roast French Press Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (1000, 2500), 'consumption': (1.5, 8), 'refill_threshold': (100, 250), 'refill_qty': (1000, 2500), 'sensor_type': 'weight_sensor_bean'},\n",
        "    5: {'name': 'Light Roast Pour Over Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (700, 1800), 'consumption': (0.8, 4), 'refill_threshold': (70, 180), 'refill_qty': (700, 1800), 'sensor_type': 'weight_sensor_bean'},\n",
        "    6: {'name': 'Nitro Cold Brew Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (1500, 4000), 'consumption': (1.4, 7), 'refill_threshold': (150, 400), 'refill_qty': (1500, 4000), 'sensor_type': 'weight_sensor_bean'},\n",
        "    7: {'name': 'Seasonal Limited Edition Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (500, 1000), 'consumption': (0.4, 2), 'refill_threshold': (50, 100), 'refill_qty': (500, 1000), 'sensor_type': 'weight_sensor_bean'},\n",
        "    8: {'name': 'Breakfast Blend Beans', 'category': 'Coffee Beans', 'uom': 'grams', 'perishable': True, 'initial_qty': (2000, 4500), 'consumption': (2, 9), 'refill_threshold': (200, 450), 'refill_qty': (2000, 4500), 'sensor_type': 'weight_sensor_bean'},\n",
        "\n",
        "    # Coffee Liquids/Concentrates (liters) - consumption per minute\n",
        "    9: {'name': 'Cold Brew Concentrate (Regular)', 'category': 'Coffee Liquid', 'uom': 'liters', 'perishable': True, 'initial_qty': (10, 20), 'consumption': (0.02, 0.1), 'refill_threshold': (1, 3), 'refill_qty': (10, 20), 'sensor_type': 'level_sensor_liquid'},\n",
        "    10: {'name': 'Nitro Cold Brew Concentrate', 'category': 'Coffee Liquid', 'uom': 'liters', 'perishable': True, 'initial_qty': (5, 15), 'consumption': (0.01, 0.06), 'refill_threshold': (0.5, 2), 'refill_qty': (5, 15), 'sensor_type': 'level_sensor_liquid'},\n",
        "\n",
        "    # Milk & Dairy Alternatives (liters) - consumption per minute\n",
        "    11: {'name': 'Whole Dairy Milk', 'category': 'Milk', 'uom': 'liters', 'perishable': True, 'initial_qty': (8, 16), 'consumption': (0.02, 0.08), 'refill_threshold': (1, 3), 'refill_qty': (8, 16), 'sensor_type': 'level_sensor_liquid'},\n",
        "    12: {'name': 'Skim Dairy Milk', 'category': 'Milk', 'uom': 'liters', 'perishable': True, 'initial_qty': (4, 8), 'consumption': (0.01, 0.04), 'refill_threshold': (0.5, 1.5), 'refill_qty': (4, 8), 'sensor_type': 'level_sensor_liquid'},\n",
        "    13: {'name': 'Organic Oat Milk', 'category': 'Milk', 'uom': 'liters', 'perishable': True, 'initial_qty': (6, 12), 'consumption': (0.02, 0.06), 'refill_threshold': (0.8, 2), 'refill_qty': (6, 12), 'sensor_type': 'level_sensor_liquid'},\n",
        "    14: {'name': 'Barista Blend Almond Milk', 'category': 'Milk', 'uom': 'liters', 'perishable': True, 'initial_qty': (4, 10), 'consumption': (0.01, 0.05), 'refill_threshold': (0.5, 1.5), 'refill_qty': (4, 10), 'sensor_type': 'level_sensor_liquid'},\n",
        "    15: {'name': 'Coconut Milk', 'category': 'Milk', 'uom': 'liters', 'perishable': True, 'initial_qty': (2, 5), 'consumption': (0.004, 0.02), 'refill_threshold': (0.2, 0.5), 'refill_qty': (2, 5), 'sensor_type': 'level_sensor_liquid'},\n",
        "    16: {'name': 'Soy Milk Unsweetened', 'category': 'Milk', 'uom': 'liters', 'perishable': True, 'initial_qty': (3, 7), 'consumption': (0.008, 0.03), 'refill_threshold': (0.3, 0.8), 'refill_qty': (3, 7), 'sensor_type': 'level_sensor_liquid'},\n",
        "\n",
        "    # Syrups & Sauces (milliliters) - consumption per minute\n",
        "    17: {'name': 'Classic Vanilla Syrup', 'category': 'Syrup', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (750, 1500), 'consumption': (2, 6), 'refill_threshold': (75, 150), 'refill_qty': (750, 1500), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    18: {'name': 'Sugar-Free Vanilla Syrup', 'category': 'Syrup', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (500, 1000), 'consumption': (1, 4), 'refill_threshold': (50, 100), 'refill_qty': (500, 1000), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    19: {'name': 'Caramel Sauce', 'category': 'Sauce', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (500, 1200), 'consumption': (1.6, 5), 'refill_threshold': (50, 120), 'refill_qty': (500, 1200), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    20: {'name': 'Chocolate Mocha Sauce', 'category': 'Sauce', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (750, 1800), 'consumption': (2.4, 8), 'refill_threshold': (75, 180), 'refill_qty': (750, 1800), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    21: {'name': 'White Chocolate Sauce', 'category': 'Sauce', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (400, 800), 'consumption': (1, 3), 'refill_threshold': (40, 80), 'refill_qty': (400, 800), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    22: {'name': 'Hazelnut Syrup', 'category': 'Syrup', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (300, 700), 'consumption': (0.6, 2), 'refill_threshold': (30, 70), 'refill_qty': (300, 700), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    23: {'name': 'Toasted Marshmallow Syrup', 'category': 'Syrup', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (200, 500), 'consumption': (0.4, 1.6), 'refill_threshold': (20, 50), 'refill_qty': (200, 500), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    24: {'name': 'Spiced Chai Concentrate', 'category': 'Syrup', 'uom': 'liters', 'perishable': True, 'initial_qty': (3, 7), 'consumption': (0.01, 0.04), 'refill_threshold': (0.3, 0.7), 'refill_qty': (3, 7), 'sensor_type': 'level_sensor_liquid'},\n",
        "    25: {'name': 'Pumpkin Spice Sauce', 'category': 'Sauce', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (200, 600), 'consumption': (1, 4), 'refill_threshold': (20, 60), 'refill_qty': (200, 600), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    26: {'name': 'Peppermint Bark Syrup', 'category': 'Syrup', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (200, 500), 'consumption': (0.8, 3), 'refill_threshold': (20, 50), 'refill_qty': (200, 500), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    27: {'name': 'Agave Nectar', 'category': 'Sweetener Liquid', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (300, 800), 'consumption': (0.6, 2.4), 'refill_threshold': (30, 80), 'refill_qty': (300, 800), 'sensor_type': 'weight_sensor_syrup'},\n",
        "    28: {'name': 'Brown Sugar Cinnamon Syrup', 'category': 'Syrup', 'uom': 'milliliters', 'perishable': True, 'initial_qty': (400, 900), 'consumption': (1.2, 5), 'refill_threshold': (40, 90), 'refill_qty': (400, 900), 'sensor_type': 'weight_sensor_syrup'},\n",
        "\n",
        "    # Other Beverages / Bases (grams) - consumption per minute\n",
        "    29: {'name': 'Hot Cocoa Mix', 'category': 'Beverage Powder', 'uom': 'grams', 'perishable': False, 'initial_qty': (500, 1500), 'consumption': (2, 10), 'refill_threshold': (50, 150), 'refill_qty': (500, 1500), 'sensor_type': 'weight_sensor_powder'},\n",
        "    30: {'name': 'Matcha Green Tea Powder', 'category': 'Tea', 'uom': 'grams', 'perishable': False, 'initial_qty': (200, 600), 'consumption': (1, 4), 'refill_threshold': (20, 60), 'refill_qty': (200, 600), 'sensor_type': 'weight_sensor_powder'},\n",
        "\n",
        "    # Consumables / Packaging (count) - consumption per minute\n",
        "    31: {'name': 'Standard 12oz Hot Cup', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (500, 1000), 'consumption': (1, 5), 'refill_threshold': (50, 100), 'refill_qty': (500, 1000), 'sensor_type': 'count_sensor_stack'},\n",
        "    32: {'name': 'Standard 16oz Hot Cup', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (400, 800), 'consumption': (0.8, 4), 'refill_threshold': (40, 80), 'refill_qty': (400, 800), 'sensor_type': 'count_sensor_stack'},\n",
        "    33: {'name': 'Standard 20oz Cold Cup', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (300, 600), 'consumption': (0.6, 3), 'refill_threshold': (30, 60), 'refill_qty': (300, 600), 'sensor_type': 'count_sensor_stack'},\n",
        "    34: {'name': 'Hot Cup Lids (Mixed Size)', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (700, 1400), 'consumption': (1.4, 7), 'refill_threshold': (70, 140), 'refill_qty': (700, 1400), 'sensor_type': 'count_sensor_stack'},\n",
        "    35: {'name': 'Cold Cup Lids (Domed)', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (250, 500), 'consumption': (0.5, 2), 'refill_threshold': (25, 50), 'refill_qty': (250, 500), 'sensor_type': 'count_sensor_stack'},\n",
        "    36: {'name': 'Coffee Sleeves', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (800, 1600), 'consumption': (1.6, 8), 'refill_threshold': (80, 160), 'refill_qty': (800, 1600), 'sensor_type': 'count_sensor_stack'},\n",
        "    37: {'name': 'Stir Sticks (Wood)', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (1000, 2000), 'consumption': (2, 10), 'refill_threshold': (100, 200), 'refill_qty': (1000, 2000), 'sensor_type': 'count_sensor_bin'},\n",
        "    38: {'name': 'Sugar Packets', 'category': 'Sweetener', 'uom': 'count', 'perishable': False, 'initial_qty': (1000, 2500), 'consumption': (3, 15), 'refill_threshold': (100, 250), 'refill_qty': (1000, 2500), 'sensor_type': 'count_sensor_bin'},\n",
        "    39: {'name': 'Splenda Sweetener Packets', 'category': 'Sweetener', 'uom': 'count', 'perishable': False, 'initial_qty': (500, 1000), 'consumption': (1, 5), 'refill_threshold': (50, 100), 'refill_qty': (500, 1000), 'sensor_type': 'count_sensor_bin'},\n",
        "    40: {'name': 'Napkins (Recycled)', 'category': 'Packaging', 'uom': 'count', 'perishable': False, 'initial_qty': (1500, 3000), 'consumption': (4, 20), 'refill_threshold': (150, 300), 'refill_qty': (1500, 3000), 'sensor_type': 'count_sensor_bin'}\n",
        "}\n",
        "\n",
        "# --- Initial State for each truck's inventory ---\n",
        "truck_inventory_states = {}\n",
        "for truck_id in range(1, NUM_TRUCKS + 1):\n",
        "    truck_inventory_states[truck_id] = {}\n",
        "    for ing_id, props in INGREDIENT_PROPERTIES.items():\n",
        "        # Initialize slightly above refill threshold to avoid immediate refill at start\n",
        "        initial_qty = random.uniform(props['refill_threshold'][1], props['initial_qty'][1])\n",
        "        truck_inventory_states[truck_id][ing_id] = initial_qty\n",
        "\n",
        "# --- CSV Header ---\n",
        "CSV_HEADER_INVENTORY = [\n",
        "    \"telemetry_inventory_id\",\n",
        "    \"telemetry_load_id\",\n",
        "    \"truck_id\",\n",
        "    \"telemetry_timestamp\",\n",
        "    \"ingredient_id\",\n",
        "    \"current_quantity_value\",\n",
        "    \"unit_of_measure\",\n",
        "    \"event_type\",\n",
        "    \"associated_transaction_id\",\n",
        "    \"source_sensor_id\"\n",
        "]\n",
        "\n",
        "# --- Main Data Generation Logic for Inventory Telemetry ---\n",
        "def generate_inventory_data():\n",
        "    \"\"\"\n",
        "    Generates telemetry data for inventory levels (1-minute granularity, 5-min batch ID).\n",
        "    \"\"\"\n",
        "    print(f\"Starting inventory data generation for {NUM_TRUCKS} trucks from {START_DATE} to {END_DATE}...\")\n",
        "    print(f\"Output file: {OUTPUT_FILENAME_INVENTORY}\")\n",
        "    print(f\"Generating data every {INTERVAL_MINUTES_READING} minute(s), with batch IDs every {INTERVAL_MINUTES_BATCH} minutes.\")\n",
        "\n",
        "\n",
        "    with open(OUTPUT_FILENAME_INVENTORY, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(CSV_HEADER_INVENTORY) # Write the header row\n",
        "\n",
        "        current_timestamp = START_DATE\n",
        "        row_count = 0\n",
        "        current_batch_load_id = None\n",
        "\n",
        "        while current_timestamp < END_DATE:\n",
        "            # Generate a new batch ID every INTERVAL_MINUTES_BATCH minutes\n",
        "            if current_timestamp.minute % INTERVAL_MINUTES_BATCH == 0 and current_timestamp.second == 0:\n",
        "                 current_batch_load_id = current_timestamp.strftime('%Y%m%d%H%M%S') + '_inventory_batch'\n",
        "\n",
        "            for truck_id in range(1, NUM_TRUCKS + 1):\n",
        "                for ing_id, props in INGREDIENT_PROPERTIES.items():\n",
        "                    current_qty = truck_inventory_states[truck_id][ing_id]\n",
        "\n",
        "                    # Simulate consumption for this interval\n",
        "                    consumption_amount = random.uniform(props['consumption'][0], props['consumption'][1])\n",
        "                    new_qty = max(0, current_qty - consumption_amount)\n",
        "\n",
        "                    # Record sensor reading for current state\n",
        "                    writer.writerow([\n",
        "                        str(uuid.uuid4()),\n",
        "                        current_batch_load_id,\n",
        "                        truck_id,\n",
        "                        current_timestamp.isoformat(),\n",
        "                        ing_id,\n",
        "                        round(new_qty, 3),\n",
        "                        props['uom'],\n",
        "                        'sensor_reading',\n",
        "                        None,\n",
        "                        props['sensor_type']\n",
        "                    ])\n",
        "                    row_count += 1\n",
        "                    truck_inventory_states[truck_id][ing_id] = new_qty\n",
        "\n",
        "                    # Check for replenishment needed\n",
        "                    if new_qty <= random.uniform(props['refill_threshold'][0], props['refill_threshold'][1]):\n",
        "                        refill_amount = random.uniform(props['refill_qty'][0], props['refill_qty'][1])\n",
        "                        replenished_qty = refill_amount + random.uniform(0, consumption_amount * 0.1)\n",
        "\n",
        "                        writer.writerow([\n",
        "                            str(uuid.uuid4()),\n",
        "                            current_batch_load_id, # Same batch ID as the sensor reading in this interval\n",
        "                            truck_id,\n",
        "                            current_timestamp.isoformat(),\n",
        "                            ing_id,\n",
        "                            round(replenished_qty, 3),\n",
        "                            props['uom'],\n",
        "                            'replenished',\n",
        "                            None,\n",
        "                            'manual_entry'\n",
        "                        ])\n",
        "                        row_count += 1\n",
        "                        truck_inventory_states[truck_id][ing_id] = replenished_qty\n",
        "\n",
        "            current_timestamp += timedelta(minutes=INTERVAL_MINUTES_READING)\n",
        "\n",
        "            # Print progress update (e.g., every simulated day's start)\n",
        "            if current_timestamp.minute == 0 and current_timestamp.hour == 0:\n",
        "                print(f\"Generated {row_count} rows up to {current_timestamp.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "    print(f\"\\nInventory telemetry generation complete! Total rows generated: {row_count}. File saved to: {OUTPUT_FILENAME_INVENTORY}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDsMseOL8tBM"
      },
      "outputs": [],
      "source": [
        "generate_inventory_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlI05lbr806u"
      },
      "outputs": [],
      "source": [
        "!head telemetry_inventory_raw.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foN0HfCy88gz"
      },
      "outputs": [],
      "source": [
        "!gsutil cp telemetry_inventory_raw.csv gs://gcs-bucket-namet/telemetry_inventory_raw/telemetry_inventory_raw.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fhDUWAa9DqY"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "LOAD DATA INTO `agentic_beans_raw.telemetry_inventory`\n",
        "(\n",
        "    telemetry_inventory_id          STRING,\n",
        "    telemetry_load_id               STRING,\n",
        "    truck_id                        INT64,\n",
        "    telemetry_timestamp             TIMESTAMP,\n",
        "    ingredient_id                   INT64,\n",
        "    current_quantity_value          NUMERIC(10, 3),\n",
        "    unit_of_measure                 STRING,\n",
        "    event_type                      STRING,\n",
        "    associated_transaction_id       STRING,\n",
        "    source_sensor_id                STRING,\n",
        ")\n",
        "FROM FILES (\n",
        " format = 'csv',\n",
        " skip_leading_rows = 1,\n",
        " uris = ['gs://gcs-bucket-namet/telemetry_inventory_raw/telemetry_inventory_raw.csv']);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBm8zws-GIf0"
      },
      "source": [
        "#### Generate Camera Telemetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY95JWfuFlix"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# --- Configuration ---\n",
        "OUTPUT_FILENAME_CAMERA = 'telemetry_camera_vision_raw.csv'\n",
        "START_DATE = datetime(2020, 1, 1, 0, 0, 0)\n",
        "END_DATE = datetime(2027, 1, 1, 0, 0, 0)\n",
        "INTERVAL_SECONDS_READING = 30 # Data reading interval is now every 30 seconds\n",
        "INTERVAL_MINUTES_BATCH = 5   # Batch upload ID is still generated every 5 minutes\n",
        "NUM_TRUCKS = 20\n",
        "\n",
        "# --- Camera ID Mapping (based on prior camera_dim inserts) ---\n",
        "# For truck_id T (1-20), the cameras are:\n",
        "# Exterior Queue Camera ID: (T * 2) - 1\n",
        "# Interior Cabin Camera ID: (T * 2)\n",
        "CAMERA_TYPES_PER_TRUCK = {\n",
        "    'exterior_queue': {'id_offset': -1}, # (truck_id * 2) - 1\n",
        "    'interior_cabin': {'id_offset': 0}   # (truck_id * 2)\n",
        "}\n",
        "\n",
        "# --- Normal Operating Ranges for AI detections (for clean data) ---\n",
        "# These are dynamic ranges to simulate natural fluctuations\n",
        "NORMAL_DETECTION_RANGES = {\n",
        "    'exterior_queue': {\n",
        "        'people_in_queue': {'min': 0, 'max': 15}, # Max typical queue length\n",
        "        'foot_traffic_nearby': {'min': 10, 'max': 100}, # General foot traffic\n",
        "        'confidence': {'min': 0.90, 'max': 0.99}, # High confidence for clean data\n",
        "        'model_version': 'AI_Vision_v2.5'\n",
        "    },\n",
        "    'interior_cabin': {\n",
        "        'people_in_queue': {'min': 0, 'max': 5}, # Internal staff/customers not necessarily in queue\n",
        "        'foot_traffic_nearby': {'min': 0, 'max': 10}, # Less relevant, but some movement\n",
        "        'confidence': {'min': 0.90, 'max': 0.99},\n",
        "        'model_version': 'AI_Vision_v2.5_Interior'\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- CSV Header ---\n",
        "CSV_HEADER_CAMERA = [\n",
        "    \"telemetry_camera_vision_id\",\n",
        "    \"telemetry_load_id\",\n",
        "    \"truck_id\",\n",
        "    \"telemetry_timestamp\",\n",
        "    \"camera_id\",\n",
        "    \"people_in_queue_count\",\n",
        "    \"foot_traffic_count_nearby\",\n",
        "    \"ai_detection_confidence_score\",\n",
        "    \"image_reference_url\",\n",
        "    \"detection_model_version\"\n",
        "]\n",
        "\n",
        "# --- Main Data Generation Logic for Camera Vision Telemetry ---\n",
        "def generate_telemetry_camera_vision_data():\n",
        "    \"\"\"\n",
        "    Generates telemetry data for camera vision (30-second granularity, 5-min batch ID).\n",
        "    \"\"\"\n",
        "    print(f\"Starting camera vision telemetry generation for {NUM_TRUCKS} trucks from {START_DATE} to {END_DATE}...\")\n",
        "    print(f\"Output file: {OUTPUT_FILENAME_CAMERA}\")\n",
        "    print(f\"Generating data every {INTERVAL_SECONDS_READING} second(s), with batch IDs every {INTERVAL_MINUTES_BATCH} minutes.\")\n",
        "\n",
        "    with open(OUTPUT_FILENAME_CAMERA, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(CSV_HEADER_CAMERA) # Write the header row\n",
        "\n",
        "        current_timestamp = START_DATE\n",
        "        row_count = 0\n",
        "        current_batch_load_id = None\n",
        "\n",
        "        while current_timestamp < END_DATE:\n",
        "            # Generate a new batch ID every INTERVAL_MINUTES_BATCH minutes\n",
        "            # This ensures the batch ID changes at 0, 5, 10, 15... minutes and 0 seconds of each hour\n",
        "            if current_timestamp.minute % INTERVAL_MINUTES_BATCH == 0 and current_timestamp.second == 0:\n",
        "                 current_batch_load_id = current_timestamp.strftime('%Y%m%d%H%M%S') + '_camera_batch'\n",
        "\n",
        "            for truck_id in range(1, NUM_TRUCKS + 1):\n",
        "                # Generate data for each camera type for this truck\n",
        "                for camera_type, params in CAMERA_TYPES_PER_TRUCK.items():\n",
        "                    camera_id = (truck_id * 2) + params['id_offset']\n",
        "                    detection_props = NORMAL_DETECTION_RANGES[camera_type]\n",
        "\n",
        "                    people_in_queue = random.randint(detection_props['people_in_queue']['min'], detection_props['people_in_queue']['max'])\n",
        "                    foot_traffic = random.randint(detection_props['foot_traffic_nearby']['min'], detection_props['foot_traffic_nearby']['max'])\n",
        "                    confidence = round(random.uniform(detection_props['confidence']['min'], detection_props['confidence']['max']), 2)\n",
        "                    model_version = detection_props['model_version']\n",
        "                    image_ref_url = None # For clean data, no specific image URL needed for demo\n",
        "\n",
        "                    # Create the row data\n",
        "                    row = [\n",
        "                        str(uuid.uuid4()), # telemetry_camera_vision_id (unique per row)\n",
        "                        current_batch_load_id, # This ID will change every 5 minutes\n",
        "                        truck_id,\n",
        "                        current_timestamp.isoformat(), # ISO format for BigQuery TIMESTAMP\n",
        "                        camera_id,\n",
        "                        people_in_queue,\n",
        "                        foot_traffic,\n",
        "                        confidence,\n",
        "                        image_ref_url,\n",
        "                        model_version\n",
        "                    ]\n",
        "                    writer.writerow(row)\n",
        "                    row_count += 1\n",
        "\n",
        "            current_timestamp += timedelta(seconds=INTERVAL_SECONDS_READING)\n",
        "\n",
        "            # Print progress update (e.g., every 100,000 rows)\n",
        "            if row_count % 100000 == 0:\n",
        "                print(f\"Generated {row_count} rows up to {current_timestamp}...\")\n",
        "\n",
        "    print(f\"\\nCamera vision telemetry generation complete! Total rows generated: {row_count}. File saved to: {OUTPUT_FILENAME_CAMERA}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyXlCVwYFlgc"
      },
      "outputs": [],
      "source": [
        "generate_telemetry_camera_vision_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYDr1MdBFldS"
      },
      "outputs": [],
      "source": [
        "!head telemetry_camera_vision_raw.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRXK0ELZFzu4"
      },
      "outputs": [],
      "source": [
        "!gsutil cp telemetry_camera_vision_raw.csv gs://gcs-bucket-namet/telemetry_camera_vision_raw/telemetry_camera_vision_raw.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6vW1_IZF6cP"
      },
      "outputs": [],
      "source": [
        "%%bigquery\n",
        "\n",
        "LOAD DATA INTO `agentic_beans_raw.telemetry_camera_vision`\n",
        "(\n",
        "    telemetry_camera_vision_id      STRING,\n",
        "    telemetry_load_id               STRING,\n",
        "    truck_id                        INT64,\n",
        "    telemetry_timestamp             TIMESTAMP,\n",
        "    camera_id                       INT64,\n",
        "    people_in_queue_count           INT64,\n",
        "    foot_traffic_count_nearby       INT64,\n",
        "    ai_detection_confidence_score   NUMERIC(3, 2),\n",
        "    image_reference_url             STRING,\n",
        "    detection_model_version         STRING,\n",
        ")\n",
        "FROM FILES (\n",
        " format = 'csv',\n",
        " skip_leading_rows = 1,\n",
        " uris = ['gs://gcs-bucket-namet/telemetry_camera_vision_raw/telemetry_camera_vision_raw.csv']);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HMsUvoF4BP7Y",
        "m65vp54BUFRi",
        "UmyL-Rg4Dr_f",
        "JbOjdSP1kN9T",
        "kTYXm6hMW2b9",
        "bI-KJELZ1jgt",
        "iPz91mshqZBb",
        "ASQ2BPisXDA0"
      ],
      "name": "Agentic Beans - Data Generation: Truck Telemetry",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
